{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dba5600d-bbdc-4aa2-a0bc-bd7331d16cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report saved to evaluation_report.txt\n",
      "RAG Evaluation Report\n",
      "===================\n",
      "\n",
      "Overall Statistics:\n",
      "-----------------\n",
      "Total Questions: 127\n",
      "Average Similarity: 0.479\n",
      "Confidence Alignment: 0.843\n",
      "\n",
      "Analysis by Confidence Level:\n",
      "--------------------------\n",
      "High Confidence:\n",
      "  Percentage: 76.4%\n",
      "  Average Similarity: 0.521\n",
      "Medium Confidence:\n",
      "  Percentage: 3.1%\n",
      "  Average Similarity: 0.251\n",
      "Low Confidence:\n",
      "  Percentage: 20.5%\n",
      "  Average Similarity: 0.358\n",
      "\n",
      "Confidence Matrix (Ground Truth vs RAG):\n",
      "-------------------------------------\n",
      "high -> high: 92\n",
      "low -> high: 4\n",
      "high -> low: 16\n",
      "high -> medium: 2\n",
      "low -> low: 7\n",
      "medium -> medium: 1\n",
      "medium -> low: 3\n",
      "medium -> high: 1\n",
      "low -> medium: 1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "class SimpleRAGEvaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the evaluator with basic text similarity.\"\"\"\n",
    "        pass\n",
    "        \n",
    "    def calculate_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate text similarity using SequenceMatcher.\"\"\"\n",
    "        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()\n",
    "    \n",
    "    def evaluate_confidence_alignment(self, gt_confidence: str, rag_confidence: str) -> bool:\n",
    "        \"\"\"Check if confidence levels are aligned.\"\"\"\n",
    "        confidence_levels = {'high': 2, 'medium': 1, 'low': 0}\n",
    "        return abs(confidence_levels[gt_confidence] - confidence_levels[rag_confidence]) <= 1\n",
    "    \n",
    "    def load_data(self, ground_truth_path: str, rag_output_path: str) -> tuple:\n",
    "        \"\"\"Load ground truth and RAG output files.\"\"\"\n",
    "        with open(ground_truth_path, 'r') as f:\n",
    "            ground_truth = json.load(f)['ground_truth']\n",
    "        \n",
    "        with open(rag_output_path, 'r') as f:\n",
    "            rag_output = json.load(f)\n",
    "            \n",
    "        return ground_truth, rag_output\n",
    "    \n",
    "    def evaluate_answers(self, ground_truth: Dict, rag_output: Dict) -> Dict:\n",
    "        \"\"\"Perform comprehensive evaluation of RAG outputs.\"\"\"\n",
    "        results = {\n",
    "            'metrics': {},\n",
    "            'details': [],\n",
    "            'confidence_matrix': defaultdict(int),\n",
    "            'similarity_by_confidence': defaultdict(list)\n",
    "        }\n",
    "        \n",
    "        for question_id in ground_truth:\n",
    "            gt = ground_truth[question_id]\n",
    "            rag = rag_output.get(question_id)\n",
    "            \n",
    "            if not rag:\n",
    "                continue\n",
    "                \n",
    "            gt_answer = gt['answer_data']['answer']\n",
    "            rag_answer = rag['answer_data']['answer']\n",
    "            gt_confidence = gt['answer_data']['confidence']\n",
    "            rag_confidence = rag['answer_data']['confidence']\n",
    "            \n",
    "            # Calculate text similarity\n",
    "            similarity = self.calculate_similarity(gt_answer, rag_answer)\n",
    "            \n",
    "            # Track confidence alignment\n",
    "            results['confidence_matrix'][f\"{gt_confidence}_{rag_confidence}\"] += 1\n",
    "            \n",
    "            # Track similarity scores by confidence\n",
    "            results['similarity_by_confidence'][rag_confidence].append(similarity)\n",
    "            \n",
    "            # Store detailed results\n",
    "            results['details'].append({\n",
    "                'question_id': question_id,\n",
    "                'question': gt['question'],\n",
    "                'ground_truth': gt_answer,\n",
    "                'rag_answer': rag_answer,\n",
    "                'similarity': similarity,\n",
    "                'gt_confidence': gt_confidence,\n",
    "                'rag_confidence': rag_confidence,\n",
    "                'confidence_aligned': self.evaluate_confidence_alignment(gt_confidence, rag_confidence)\n",
    "            })\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total = len(results['details'])\n",
    "        \n",
    "        results['metrics'] = {\n",
    "            'total_questions': total,\n",
    "            'avg_similarity': np.mean([d['similarity'] for d in results['details']]),\n",
    "            'confidence_alignment': sum(1 for d in results['details'] if d['confidence_aligned']) / total,\n",
    "            'confidence_distribution': {\n",
    "                conf: len(scores) / total \n",
    "                for conf, scores in results['similarity_by_confidence'].items()\n",
    "            },\n",
    "            'avg_similarity_by_confidence': {\n",
    "                conf: np.mean(scores) \n",
    "                for conf, scores in results['similarity_by_confidence'].items()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_report(self, results: Dict, output_path: Optional[str] = None) -> str:\n",
    "        \"\"\"Generate a detailed evaluation report.\"\"\"\n",
    "        report = []\n",
    "        report.append(\"RAG Evaluation Report\")\n",
    "        report.append(\"===================\\n\")\n",
    "        \n",
    "        metrics = results['metrics']\n",
    "        \n",
    "        # Overall Statistics\n",
    "        report.append(\"Overall Statistics:\")\n",
    "        report.append(\"-----------------\")\n",
    "        report.append(f\"Total Questions: {metrics['total_questions']}\")\n",
    "        report.append(f\"Average Similarity: {metrics['avg_similarity']:.3f}\")\n",
    "        report.append(f\"Confidence Alignment: {metrics['confidence_alignment']:.3f}\\n\")\n",
    "        \n",
    "        # Confidence Level Analysis\n",
    "        report.append(\"Analysis by Confidence Level:\")\n",
    "        report.append(\"--------------------------\")\n",
    "        for confidence in ['high', 'medium', 'low']:\n",
    "            if confidence in metrics['confidence_distribution']:\n",
    "                dist = metrics['confidence_distribution'][confidence]\n",
    "                avg_sim = metrics['avg_similarity_by_confidence'][confidence]\n",
    "                report.append(f\"{confidence.capitalize()} Confidence:\")\n",
    "                report.append(f\"  Percentage: {dist:.1%}\")\n",
    "                report.append(f\"  Average Similarity: {avg_sim:.3f}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Confidence Matrix\n",
    "        report.append(\"Confidence Matrix (Ground Truth vs RAG):\")\n",
    "        report.append(\"-------------------------------------\")\n",
    "        for pair, count in results['confidence_matrix'].items():\n",
    "            gt_conf, rag_conf = pair.split('_')\n",
    "            report.append(f\"{gt_conf} -> {rag_conf}: {count}\")\n",
    "        \n",
    "        report_text = \"\\n\".join(report)\n",
    "        \n",
    "        if output_path:\n",
    "            with open(output_path, 'w') as f:\n",
    "                f.write(report_text)\n",
    "            print(f\"Report saved to {output_path}\")\n",
    "        \n",
    "        return report_text\n",
    "\n",
    "def evaluate_rag_system(ground_truth_path: str, rag_output_path: str, report_path: Optional[str] = None):\n",
    "    \"\"\"Convenience function to run full evaluation.\"\"\"\n",
    "    evaluator = SimpleRAGEvaluator()\n",
    "    ground_truth, rag_output = evaluator.load_data(ground_truth_path, rag_output_path)\n",
    "    results = evaluator.evaluate_answers(ground_truth, rag_output)\n",
    "    report = evaluator.generate_report(results, report_path)\n",
    "    return results, report\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    results, report = evaluate_rag_system(\n",
    "        ground_truth_path=\"data/ground_truth_2000.json\",\n",
    "        rag_output_path=\"rag_outputs.json\",\n",
    "        report_path=\"evaluation_report.txt\"\n",
    "    )\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "111bbcd2-4a60-4968-b247-5269f001c597",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced report saved to enhanced_evaluation_report.txt\n",
      "Enhanced RAG Evaluation Report\n",
      "===========================\n",
      "\n",
      "1. Basic Statistics:\n",
      "------------------\n",
      "Total Questions: 127\n",
      "Average Similarity: 0.479\n",
      "Confidence Alignment: 0.843\n",
      "\n",
      "2. Confidence Level Analysis:\n",
      "--------------------------\n",
      "High Confidence:\n",
      "  Percentage: 76.4%\n",
      "  Average Similarity: 0.521\n",
      "  Precision: 0.948\n",
      "  Recall: 0.836\n",
      "  F1 Score: 0.889\n",
      "Medium Confidence:\n",
      "  Percentage: 3.1%\n",
      "  Average Similarity: 0.251\n",
      "  Precision: 0.250\n",
      "  Recall: 0.200\n",
      "  F1 Score: 0.222\n",
      "Low Confidence:\n",
      "  Percentage: 20.5%\n",
      "  Average Similarity: 0.358\n",
      "  Precision: 0.269\n",
      "  Recall: 0.583\n",
      "  F1 Score: 0.368\n",
      "\n",
      "3. Similarity Score Distribution:\n",
      "------------------------------\n",
      "Range 0-0.2: 37 questions\n",
      "Range 0.2-0.4: 24 questions\n",
      "Range 0.4-0.6: 15 questions\n",
      "Range 0.6-0.8: 22 questions\n",
      "Range 0.8-1.0: 29 questions\n",
      "\n",
      "4. Accuracy at Different Thresholds:\n",
      "----------------------------------\n",
      "Threshold 0.3: 0.567\n",
      "Threshold 0.5: 0.472\n",
      "Threshold 0.7: 0.315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "class EnhancedRAGEvaluator(SimpleRAGEvaluator):\n",
    "    def calculate_advanced_metrics(self, results: Dict) -> Dict:\n",
    "        \"\"\"Calculate additional advanced metrics.\"\"\"\n",
    "        details = results['details']\n",
    "        \n",
    "        # Calculate precision, recall, and F1 for confidence levels\n",
    "        y_true = [d['gt_confidence'] for d in details]\n",
    "        y_pred = [d['rag_confidence'] for d in details]\n",
    "        \n",
    "        labels = ['high', 'medium', 'low']\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            y_true, y_pred, labels=labels, average=None\n",
    "        )\n",
    "        \n",
    "        # Calculate thresholded accuracy\n",
    "        thresholds = [0.3, 0.5, 0.7]\n",
    "        accuracy_at_threshold = {\n",
    "            f\"accuracy_{t}\": sum(1 for d in details if d['similarity'] >= t) / len(details)\n",
    "            for t in thresholds\n",
    "        }\n",
    "        \n",
    "        # Calculate error analysis\n",
    "        similarity_bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "        similarity_distribution = pd.cut(\n",
    "            [d['similarity'] for d in details], \n",
    "            bins=similarity_bins\n",
    "        ).value_counts().sort_index()\n",
    "        \n",
    "        # Advanced confidence analysis\n",
    "        conf_matrix = confusion_matrix(\n",
    "            y_true, y_pred, labels=labels\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'precision_by_confidence': {\n",
    "                label: p for label, p in zip(labels, precision)\n",
    "            },\n",
    "            'recall_by_confidence': {\n",
    "                label: r for label, r in zip(labels, recall)\n",
    "            },\n",
    "            'f1_by_confidence': {\n",
    "                label: f for label, f in zip(labels, f1)\n",
    "            },\n",
    "            'support_by_confidence': {\n",
    "                label: s for label, s in zip(labels, support)\n",
    "            },\n",
    "            'accuracy_at_threshold': accuracy_at_threshold,\n",
    "            'similarity_distribution': {\n",
    "                f\"{similarity_bins[i]}-{similarity_bins[i+1]}\": count \n",
    "                for i, count in enumerate(similarity_distribution)\n",
    "            },\n",
    "            'confusion_matrix': conf_matrix.tolist()\n",
    "        }\n",
    "    \n",
    "    def generate_enhanced_report(self, results: Dict, advanced_metrics: Dict, output_path: Optional[str] = None) -> str:\n",
    "        \"\"\"Generate an enhanced evaluation report with advanced metrics.\"\"\"\n",
    "        report = []\n",
    "        report.append(\"Enhanced RAG Evaluation Report\")\n",
    "        report.append(\"===========================\\n\")\n",
    "        \n",
    "        # Original metrics\n",
    "        metrics = results['metrics']\n",
    "        report.append(\"1. Basic Statistics:\")\n",
    "        report.append(\"------------------\")\n",
    "        report.append(f\"Total Questions: {metrics['total_questions']}\")\n",
    "        report.append(f\"Average Similarity: {metrics['avg_similarity']:.3f}\")\n",
    "        report.append(f\"Confidence Alignment: {metrics['confidence_alignment']:.3f}\\n\")\n",
    "        \n",
    "        # Confidence Level Analysis\n",
    "        report.append(\"2. Confidence Level Analysis:\")\n",
    "        report.append(\"--------------------------\")\n",
    "        for confidence in ['high', 'medium', 'low']:\n",
    "            if confidence in metrics['confidence_distribution']:\n",
    "                dist = metrics['confidence_distribution'][confidence]\n",
    "                avg_sim = metrics['avg_similarity_by_confidence'][confidence]\n",
    "                precision = advanced_metrics['precision_by_confidence'][confidence]\n",
    "                recall = advanced_metrics['recall_by_confidence'][confidence]\n",
    "                f1 = advanced_metrics['f1_by_confidence'][confidence]\n",
    "                \n",
    "                report.append(f\"{confidence.capitalize()} Confidence:\")\n",
    "                report.append(f\"  Percentage: {dist:.1%}\")\n",
    "                report.append(f\"  Average Similarity: {avg_sim:.3f}\")\n",
    "                report.append(f\"  Precision: {precision:.3f}\")\n",
    "                report.append(f\"  Recall: {recall:.3f}\")\n",
    "                report.append(f\"  F1 Score: {f1:.3f}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Similarity Distribution\n",
    "        report.append(\"3. Similarity Score Distribution:\")\n",
    "        report.append(\"------------------------------\")\n",
    "        for range_str, count in advanced_metrics['similarity_distribution'].items():\n",
    "            report.append(f\"Range {range_str}: {count} questions\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Accuracy at Different Thresholds\n",
    "        report.append(\"4. Accuracy at Different Thresholds:\")\n",
    "        report.append(\"----------------------------------\")\n",
    "        for threshold, accuracy in advanced_metrics['accuracy_at_threshold'].items():\n",
    "            report.append(f\"Threshold {threshold.split('_')[1]}: {accuracy:.3f}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        report_text = \"\\n\".join(report)\n",
    "        \n",
    "        if output_path:\n",
    "            with open(output_path, 'w') as f:\n",
    "                f.write(report_text)\n",
    "            print(f\"Enhanced report saved to {output_path}\")\n",
    "        \n",
    "        return report_text\n",
    "\n",
    "def evaluate_rag_system_enhanced(ground_truth_path: str, rag_output_path: str, report_path: Optional[str] = None):\n",
    "    \"\"\"Run enhanced evaluation with additional metrics.\"\"\"\n",
    "    evaluator = EnhancedRAGEvaluator()\n",
    "    ground_truth, rag_output = evaluator.load_data(ground_truth_path, rag_output_path)\n",
    "    results = evaluator.evaluate_answers(ground_truth, rag_output)\n",
    "    advanced_metrics = evaluator.calculate_advanced_metrics(results)\n",
    "    report = evaluator.generate_enhanced_report(results, advanced_metrics, report_path)\n",
    "    return results, advanced_metrics, report\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    results, advanced_metrics, report = evaluate_rag_system_enhanced(\n",
    "        ground_truth_path=\"data/ground_truth_2000.json\",\n",
    "        rag_output_path=\"rag_outputs.json\",\n",
    "        report_path=\"enhanced_evaluation_report.txt\"\n",
    "    )\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "110d3610-3904-4597-9c0a-5d5cb44b01d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing cases where Ground Truth is high confidence but RAG is low confidence\n",
      "Found 16 cases\n",
      "\n",
      "Showing up to 20 examples:\n",
      "================================================================================\n",
      "\n",
      "Example 1/16:\n",
      "\n",
      "Question: What steps should I take if I am admitted to the Master's in Applied Data Science program at the University of Chicago?\n",
      "\n",
      "Ground Truth Answer (Confidence: high):\n",
      "If you are admitted to the Master's in Applied Data Science program at the University of Chicago, you should have your official e-transcripts sent to applieddatascience-admissions@uchicago.edu. If your institution cannot send your documents electronically, you should have them send your transcripts to the following mailing address: The University of Chicago, Attention: MS in Applied Data Science Admissions, 455 N Cityfront Plaza Dr., Suite 950, Chicago, Illinois 60611.\n",
      "Reasoning: The context provides specific instructions for admitted students regarding the submission of official transcripts, including both electronic and physical mailing options.\n",
      "\n",
      "RAG Answer (Confidence: low):\n",
      "Cannot be answered from the given context\n",
      "Reasoning: The provided context lacks sufficient details to answer this inquiry\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Example 2/16:\n",
      "\n",
      "Question: What are the career outcomes for graduates of the MS in Applied Data Science program compared to those from the Computer Science program?\n",
      "\n",
      "Ground Truth Answer (Confidence: high):\n",
      "The career outcomes for graduates of the MS in Applied Data Science program include roles such as Data Scientist (most common), Senior Data Science Consultant, Business Intelligence (BI) Director, Data Visualization Manager, Data Analytics Engineer, and AI Solution Architect. In contrast, the outcomes for graduates from the Computer Science program (MPCS) include positions such as Software Engineer (Developer), Senior Software Engineering Management, Software/Hardware Architect, and Senior Cyber Security Engineer.\n",
      "Reasoning: The context provides specific details about the career outcomes for both the MS in Applied Data Science program and the Computer Science program, allowing for a clear comparison between the two.\n",
      "\n",
      "RAG Answer (Confidence: low):\n",
      "Cannot be answered from the given context\n",
      "Reasoning: The provided context lacks sufficient details to answer this inquiry as it only provides information about the career outcomes for graduates of the MS in Applied Data Science program and does not include any information about the career outcomes for graduates of the Computer Science program.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Example 3/16:\n",
      "\n",
      "Question: What are the options for financial aid if I'm an international student applying to the MS in Applied Data Science program?\n",
      "\n",
      "Ground Truth Answer (Confidence: high):\n",
      "International students applying to the MS in Applied Data Science program can explore financial aid options by visiting the International Student Funding Options page. Additionally, they can investigate scholarships offered through various civic and professional organizations, foundations, and state agencies. Merit scholarships are also available for eligible applicants, and all applicants are automatically considered for these scholarships upon applying to the program. For further assistance, international students can work with the University of Chicago’s Student Loan Administration office once admitted.\n",
      "Reasoning: The context provides specific resources and steps for international students to explore financial aid options, including a dedicated page for international student funding, automatic consideration for merit scholarships, and the possibility of working with the Student Loan Administration office.\n",
      "\n",
      "RAG Answer (Confidence: low):\n",
      "Cannot be answered from the given context\n",
      "Reasoning: The provided context lacks sufficient details to answer this inquiry\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Example 4/16:\n",
      "\n",
      "Question: What practical applications can I expect to work on in the reinforcement learning course?\n",
      "\n",
      "Ground Truth Answer (Confidence: high):\n",
      "In the reinforcement learning course, you can expect to work on practical applications that involve a system or agent interacting with an environment to learn optimal behaviors. The course will cover both basic and advanced concepts in reinforcement learning, focusing on applications that involve time, actions, states, uncertainty, and rewards. You will learn to use neural network approximations and dynamic programming to address the 'curse of dimensionality' and improve performance in complex stochastic decision processes and intelligent control.\n",
      "Reasoning: The context provides specific details about the reinforcement learning course, including the focus on practical applications involving system-environment interactions, the use of neural networks and dynamic programming, and the goal of mastering techniques for complex decision processes.\n",
      "\n",
      "RAG Answer (Confidence: low):\n",
      "Cannot be answered from the given context\n",
      "Reasoning: The provided context lacks sufficient details to answer this inquiry\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Example 5/16:\n",
      "\n",
      "Question: What are the key topics covered in the Machine Learning I course, and how does it prepare students for data science projects?\n",
      "\n",
      "Ground Truth Answer (Confidence: high):\n",
      "The Machine Learning I course covers key topics such as data mining techniques and algorithms, exploratory analyses for uncovering and detecting patterns in multivariate data, hypothesizing and detecting relationships among variables, conducting confirmatory analyses, and building models for predictive and descriptive purposes. It provides a rigorous methodological foundation in analytical and software tools, which prepares students to successfully undertake data science projects by balancing predictive and descriptive accuracies.\n",
      "Reasoning: The context provides specific details about the topics covered in the Machine Learning I course and explains how these topics equip students with the necessary skills and knowledge for data science projects.\n",
      "\n",
      "RAG Answer (Confidence: low):\n",
      "Cannot be answered from the given context\n",
      "Reasoning: The provided context lacks sufficient details to answer this inquiry\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Example 6/16:\n",
      "\n",
      "Question: What kind of courses does the University of Chicago's MS in Applied Data Science program offer, and who are some of the instructors?\n",
      "\n",
      "Ground Truth Answer (Confidence: high):\n",
      "The University of Chicago's MS in Applied Data Science program offers courses in reinforcement learning, statistical analysis, non-linear statistical modeling, and optimization methods. One of the instructors is an individual with a PhD in Economics from the University of California, Santa Barbara, who has prior teaching experience at Northwestern University and the University of California, Santa Barbara and Riverside.\n",
      "Reasoning: The context provides specific information about the courses offered in the program and mentions an instructor with relevant teaching experience and qualifications.\n",
      "\n",
      "RAG Answer (Confidence: low):\n",
      "Cannot be answered from the given context\n",
      "Reasoning: The provided context lacks sufficient details to answer this inquiry. It mentions the program's flexibility, application deadlines, and capstone projects but does not specify the courses offered or the instructors involved.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Example 7/16:\n",
      "\n",
      "Question: What kind of generative AI tools is Nick Kadochnikov developing to help law firms and legal teams?\n",
      "\n",
      "Ground Truth Answer (Confidence: high):\n",
      "Nick Kadochnikov is developing cutting-edge Generative AI tools and advanced tech solutions that assist law firms and corporate legal teams in enhancing their service quality, making legal processes more streamlined, accessible, and requiring less manual effort.\n",
      "Reasoning: The context explicitly states that Nick Kadochnikov is at the forefront of creating Generative AI tools and advanced tech solutions aimed at improving the efficiency and accessibility of legal processes for law firms and corporate legal teams.\n",
      "\n",
      "RAG Answer (Confidence: low):\n",
      "Cannot be answered from the given context\n",
      "Reasoning: The provided context mentions that Nick Kadochnikov is at the forefront of creating cutting-edge Generative AI tools and advanced tech solutions to assist law firms and corporate legal teams, but it does not provide specific details about the kind of generative AI tools being developed.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Example 8/16:\n",
      "\n",
      "Question: What courses does Dr. Gizem Agar teach in the MS in Applied Data Science program at the University of Chicago?\n",
      "\n",
      "Ground Truth Answer (Confidence: high):\n",
      "Dr. Gizem Agar teaches Principles of Data Mining, Python for Analytics, Supply Chain Optimization, and Capstone courses in the MS in Applied Data Science program at the University of Chicago.\n",
      "Reasoning: The context explicitly lists the courses that Dr. Gizem Agar teaches, providing a clear and direct answer to the question.\n",
      "\n",
      "RAG Answer (Confidence: low):\n",
      "Cannot be answered from the given context\n",
      "Reasoning: The provided context lacks sufficient details to answer this inquiry\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Example 9/16:\n",
      "\n",
      "Question: What innovative approaches are being used by Sanjay Boddhu at HERE Technologies to enhance the creation and maintenance of digital maps?\n",
      "\n",
      "Ground Truth Answer (Confidence: high):\n",
      "Sanjay Boddhu is leading the UniMap Automation initiative at HERE Technologies, which uses AI and machine learning to transform raw spatial and nonspatial data from various sources, such as imagery, probe data, car camera feeds, lidar, and IoT data, into an actionable, navigable digital map that is updated in near real-time. This involves designing and deploying algorithms at scale and managing product roadmaps and stakeholder engagements to drive innovation and excellence in map automation and computer vision.\n",
      "Reasoning: The context provides specific details about the UniMap Automation initiative led by Sanjay Boddhu, highlighting the use of AI and machine learning to process diverse data sources for real-time digital map updates, which directly addresses the question about innovative approaches in map creation and maintenance.\n",
      "\n",
      "RAG Answer (Confidence: low):\n",
      "Cannot be answered from the given context\n",
      "Reasoning: The provided context lacks sufficient details to answer this inquiry\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Example 10/16:\n",
      "\n",
      "Question: What kind of machine learning models does Fan Yang build in his role at Discover Financial Service?\n",
      "\n",
      "Ground Truth Answer (Confidence: high):\n",
      "Fan Yang builds machine learning models for card acquisition risk modeling at Discover Financial Service.\n",
      "Reasoning: The context explicitly states that Fan Yang's role at Discover Financial Service involves building machine learning models as part of his responsibilities in leading the Card Acquisition Risk Modeling team.\n",
      "\n",
      "RAG Answer (Confidence: low):\n",
      "Cannot be answered from the given context\n",
      "Reasoning: The provided context lacks sufficient details to answer this inquiry\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Example 11/16:\n",
      "\n",
      "Question: What kind of practical applications can I expect to work on in the reinforcement learning course?\n",
      "\n",
      "Ground Truth Answer (Confidence: high):\n",
      "In the reinforcement learning course, you can expect to work on practical applications that involve developing optimal behaviors or policies for systems or agents interacting with complex environments. These applications will likely include scenarios with time, actions, states, uncertainty, and rewards, where neural network approximations and dynamic programming are used to overcome challenges such as the 'curse of dimensionality'.\n",
      "Reasoning: The context provides specific details about the reinforcement learning course, including its focus on practical applications involving complex environments and the use of neural networks and dynamic programming. This information directly addresses the inquiry about the kind of practical applications students can expect to work on.\n",
      "\n",
      "RAG Answer (Confidence: low):\n",
      "Cannot be answered from the given context\n",
      "Reasoning: The provided context lacks sufficient details to answer this inquiry\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Example 12/16:\n",
      "\n",
      "Question: What kind of hands-on projects can I expect to work on in the Real Time Intelligent Systems course?\n",
      "\n",
      "Ground Truth Answer (Confidence: high):\n",
      "In the Real Time Intelligent Systems course, you can expect to work on hands-on projects that focus on developing end-to-end analytical solutions in areas such as finance and trading, blockchains and cryptocurrencies, image recognition, and video.\n",
      "Reasoning: The context explicitly mentions that the later sessions of the course are project-based and will focus on these specific areas, providing a clear indication of the types of projects students will engage in.\n",
      "\n",
      "RAG Answer (Confidence: low):\n",
      "Cannot be answered from the given context\n",
      "Reasoning: The provided context lacks sufficient details to answer this inquiry\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Example 13/16:\n",
      "\n",
      "Question: What kind of projects will I work on in the sessions focused on end-to-end analytical solutions?\n",
      "\n",
      "Ground Truth Answer (Confidence: high):\n",
      "In the sessions focused on end-to-end analytical solutions, you will work on projects in the areas of finance and trading, blockchains and cryptocurrencies, image recognition, and video surveillance systems.\n",
      "Reasoning: The context explicitly lists the areas of finance and trading, blockchains and cryptocurrencies, image recognition, and video surveillance systems as the focus for project-based sessions developing end-to-end analytical solutions.\n",
      "\n",
      "RAG Answer (Confidence: low):\n",
      "Cannot be answered from the given context\n",
      "Reasoning: The provided context lacks sufficient details to answer this inquiry\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Example 14/16:\n",
      "\n",
      "Question: What kind of career opportunities can I expect after completing the MS in Applied Data Science program at the University of Chicago?\n",
      "\n",
      "Ground Truth Answer (Confidence: high):\n",
      "Graduates of the University of Chicago’s MS in Applied Data Science program can expect remarkable career opportunities across various industries. They have advanced skills in data science, machine learning, and data storytelling, which enable them to make impactful contributions in top organizations such as JPMorganChase, Google, and Amazon. The program's alumni have pursued diverse career paths and are actively sought after by leading industries. Additionally, 40% of full-time in-person students who had an internship accepted a full-time position with the company they interned with after graduation. The program's graduates work at notable companies including Google, TikTok, AbbVie, McDonald’s, AIG, Aon, Aetna, RTX (Raytheon Technologies), and University of Chicago Medicine.\n",
      "Reasoning: The context provides detailed information about the career outcomes for graduates of the MS in Applied Data Science program, including specific companies where alumni have found employment and the high rate of internship-to-full-time conversion, supporting a high confidence level in the answer.\n",
      "\n",
      "RAG Answer (Confidence: low):\n",
      "Cannot be answered from the given context\n",
      "Reasoning: The provided context lacks sufficient details to answer this inquiry\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Example 15/16:\n",
      "\n",
      "Question: What kind of career opportunities and salary can I expect after graduating from the MS in Applied Data Science program?\n",
      "\n",
      "Ground Truth Answer (Confidence: high):\n",
      "Graduates of the MS in Applied Data Science program can expect to work at prestigious companies such as Google, JPMorganChase, TikTok, AbbVie, McDonald’s, AIG, Aon, Aetna, RTX (Raytheon Technologies), and University of Chicago Medicine. The median post-graduation starting salary for these graduates is $115,000.\n",
      "Reasoning: The context provides specific information about the employers of program graduates and the median starting salary, which directly addresses the inquiry about career opportunities and salary expectations.\n",
      "\n",
      "RAG Answer (Confidence: low):\n",
      "Cannot be answered from the given context\n",
      "Reasoning: The provided context lacks sufficient details to answer this inquiry\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Example 16/16:\n",
      "\n",
      "Question: What happens if I miss the application deadline for the MS in Applied Data Science program?\n",
      "\n",
      "Ground Truth Answer (Confidence: high):\n",
      "If you miss the application deadline for the MS in Applied Data Science program, your application will not be considered for that respective round, as there are no exceptions to the deadline policy.\n",
      "Reasoning: The context clearly states that applications must be submitted by the deadline to be considered for each respective round, with no exceptions, indicating that missing the deadline means the application will not be reviewed.\n",
      "\n",
      "RAG Answer (Confidence: low):\n",
      "Cannot be answered from the given context\n",
      "Reasoning: The provided context lacks sufficient details to answer this inquiry\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Dict, List\n",
    "from pprint import pprint\n",
    "\n",
    "def analyze_confidence_mismatches(ground_truth_path: str, rag_output_path: str, \n",
    "                                gt_confidence: str = \"high\", rag_confidence: str = \"low\",\n",
    "                                max_examples: int = 5):\n",
    "    \"\"\"\n",
    "    Analyze cases where ground truth and RAG confidence levels mismatch.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth_path: Path to ground truth JSON\n",
    "        rag_output_path: Path to RAG output JSON\n",
    "        gt_confidence: Expected ground truth confidence level\n",
    "        rag_confidence: Expected RAG confidence level\n",
    "        max_examples: Maximum number of examples to show\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    with open(ground_truth_path, 'r') as f:\n",
    "        ground_truth = json.load(f)['ground_truth']\n",
    "    \n",
    "    with open(rag_output_path, 'r') as f:\n",
    "        rag_output = json.load(f)\n",
    "    \n",
    "    # Find mismatches\n",
    "    mismatches = []\n",
    "    for question_id in ground_truth:\n",
    "        if question_id not in rag_output:\n",
    "            continue\n",
    "            \n",
    "        gt_data = ground_truth[question_id]\n",
    "        rag_data = rag_output[question_id]\n",
    "        \n",
    "        if (gt_data['answer_data']['confidence'] == gt_confidence and \n",
    "            rag_data['answer_data']['confidence'] == rag_confidence):\n",
    "            \n",
    "            mismatches.append({\n",
    "                'question_id': question_id,\n",
    "                'question': gt_data['question'],\n",
    "                'ground_truth': {\n",
    "                    'answer': gt_data['answer_data']['answer'],\n",
    "                    'confidence': gt_data['answer_data']['confidence'],\n",
    "                    'reasoning': gt_data['answer_data']['reasoning']\n",
    "                },\n",
    "                'rag_output': {\n",
    "                    'answer': rag_data['answer_data']['answer'],\n",
    "                    'confidence': rag_data['answer_data']['confidence'],\n",
    "                    'reasoning': rag_data['answer_data']['reasoning']\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    # Print analysis\n",
    "    print(f\"\\nAnalyzing cases where Ground Truth is {gt_confidence} confidence but RAG is {rag_confidence} confidence\")\n",
    "    print(f\"Found {len(mismatches)} cases\")\n",
    "    print(\"\\nShowing up to {max_examples} examples:\".format(max_examples=max_examples))\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, mismatch in enumerate(mismatches[:max_examples]):\n",
    "        print(f\"\\nExample {i+1}/{min(max_examples, len(mismatches))}:\")\n",
    "        print(f\"\\nQuestion: {mismatch['question']}\")\n",
    "        print(\"\\nGround Truth Answer (Confidence: {}):\\n{}\\nReasoning: {}\".format(\n",
    "            mismatch['ground_truth']['confidence'],\n",
    "            mismatch['ground_truth']['answer'],\n",
    "            mismatch['ground_truth']['reasoning']\n",
    "        ))\n",
    "        print(\"\\nRAG Answer (Confidence: {}):\\n{}\\nReasoning: {}\".format(\n",
    "            mismatch['rag_output']['confidence'],\n",
    "            mismatch['rag_output']['answer'],\n",
    "            mismatch['rag_output']['reasoning']\n",
    "        ))\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    return mismatches\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    mismatches = analyze_confidence_mismatches(\n",
    "        ground_truth_path=\"data/ground_truth_2000.json\",\n",
    "        rag_output_path=\"rag_outputs.json\",\n",
    "        gt_confidence=\"high\",\n",
    "        rag_confidence=\"low\",\n",
    "        max_examples=20\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "935da420-28e7-4b6e-a28f-34b34bbea080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG System Evaluation Report\n",
      "==========================\n",
      "\n",
      "Overall Performance:\n",
      "-----------------\n",
      "answer_similarity: 0.528\n",
      "factual_accuracy: 0.983\n",
      "completeness: 1.064\n",
      "key_points_match: 0.608\n",
      "\n",
      "Performance by Category:\n",
      "----------------------\n",
      "\n",
      "Curriculum:\n",
      "  answer_similarity: 0.504\n",
      "  factual_accuracy: 0.974\n",
      "  completeness: 1.091\n",
      "  key_points_match: 0.598\n",
      "\n",
      "Admissions:\n",
      "  answer_similarity: 0.519\n",
      "  factual_accuracy: 1.000\n",
      "  completeness: 1.113\n",
      "  key_points_match: 0.588\n",
      "\n",
      "Career:\n",
      "  answer_similarity: 0.744\n",
      "  factual_accuracy: 1.000\n",
      "  completeness: 0.944\n",
      "  key_points_match: 0.751\n",
      "\n",
      "Other:\n",
      "  answer_similarity: 0.550\n",
      "  factual_accuracy: 1.000\n",
      "  completeness: 0.949\n",
      "  key_points_match: 0.612\n",
      "\n",
      "Logistics:\n",
      "  answer_similarity: 0.840\n",
      "  factual_accuracy: 1.000\n",
      "  completeness: 1.000\n",
      "  key_points_match: 0.860\n",
      "\n",
      "Financial:\n",
      "  answer_similarity: 0.433\n",
      "  factual_accuracy: 1.000\n",
      "  completeness: 1.000\n",
      "  key_points_match: 0.454\n",
      "\n",
      "Error Analysis:\n",
      "--------------\n",
      "low_similarity: 84 instances\n",
      "incomplete_answers: 26 instances\n",
      "factual_errors: 2 instances\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "@dataclass\n",
    "class EvaluationMetrics:\n",
    "    \"\"\"Stores evaluation metrics for a single answer\"\"\"\n",
    "    answer_similarity: float\n",
    "    factual_accuracy: float\n",
    "    completeness: float\n",
    "    key_points_match: float\n",
    "    category: str\n",
    "\n",
    "class RobustRAGEvaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the evaluator with program-specific evaluation logic\"\"\"\n",
    "        # Program-specific categories and expected information\n",
    "        self.categories = {\n",
    "            'admissions': ['deadline', 'apply', 'application', 'requirements', 'admission'],\n",
    "            'curriculum': ['course', 'program', 'study', 'classes', 'requirement'],\n",
    "            'logistics': ['online', 'in-person', 'schedule', 'time', 'duration'],\n",
    "            'financial': ['cost', 'tuition', 'scholarship', 'aid', 'financial'],\n",
    "            'career': ['job', 'career', 'employment', 'opportunity', 'industry'],\n",
    "            'faculty': ['professor', 'instructor', 'faculty', 'teacher', 'staff'],\n",
    "        }\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            filename=f'rag_evaluation_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log',\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        # Remove special characters but keep important punctuation\n",
    "        text = re.sub(r'[^\\w\\s.,?!-]', '', text)\n",
    "        return text\n",
    "    \n",
    "    def extract_key_points(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract key points from text\"\"\"\n",
    "        # Split on sentences and bullet points\n",
    "        points = re.split(r'[.•\\n]', text)\n",
    "        # Clean points\n",
    "        points = [self.preprocess_text(p) for p in points]\n",
    "        # Filter empty points\n",
    "        points = [p for p in points if p and len(p.split()) > 3]\n",
    "        return points\n",
    "    \n",
    "    def extract_numerical_info(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract numerical information from text\"\"\"\n",
    "        # Find all numbers and associated context\n",
    "        number_patterns = [\n",
    "            r'\\$[\\d,]+(?:\\.\\d{2})?',  # Money\n",
    "            r'\\d+(?:\\.\\d+)?%',        # Percentages\n",
    "            r'\\d+\\s+(?:hour|day|week|month|year)s?',  # Time periods\n",
    "            r'\\d+(?:st|nd|rd|th)',    # Ordinals\n",
    "            r'\\d+\\s+credit'           # Credits\n",
    "        ]\n",
    "        \n",
    "        numerical_info = []\n",
    "        for pattern in number_patterns:\n",
    "            matches = re.finditer(pattern, text.lower())\n",
    "            for match in matches:\n",
    "                # Get some context around the number\n",
    "                start = max(0, match.start() - 20)\n",
    "                end = min(len(text), match.end() + 20)\n",
    "                numerical_info.append(text[start:end].strip())\n",
    "        \n",
    "        return numerical_info\n",
    "    \n",
    "    def calculate_similarity_matrix(self, list1: List[str], list2: List[str]) -> np.ndarray:\n",
    "        \"\"\"Calculate similarity matrix between two lists of strings\"\"\"\n",
    "        matrix = np.zeros((len(list1), len(list2)))\n",
    "        for i, item1 in enumerate(list1):\n",
    "            for j, item2 in enumerate(list2):\n",
    "                matrix[i,j] = self._text_similarity(item1, item2)\n",
    "        return matrix\n",
    "    \n",
    "    def _text_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate text similarity using a combination of methods\"\"\"\n",
    "        # Normalize texts\n",
    "        text1 = self.preprocess_text(text1)\n",
    "        text2 = self.preprocess_text(text2)\n",
    "        \n",
    "        # Word overlap score\n",
    "        words1 = set(text1.split())\n",
    "        words2 = set(text2.split())\n",
    "        overlap = len(words1.intersection(words2)) / max(len(words1), len(words2))\n",
    "        \n",
    "        # Sequence similarity score\n",
    "        from difflib import SequenceMatcher\n",
    "        sequence_sim = SequenceMatcher(None, text1, text2).ratio()\n",
    "        \n",
    "        # Combine scores with weights\n",
    "        return 0.4 * overlap + 0.6 * sequence_sim\n",
    "    \n",
    "    def evaluate_answer(self, \n",
    "                       question: str, \n",
    "                       gt_answer: str, \n",
    "                       rag_answer: str) -> EvaluationMetrics:\n",
    "        \"\"\"Evaluate a single answer comprehensively\"\"\"\n",
    "        # Extract key points\n",
    "        gt_points = self.extract_key_points(gt_answer)\n",
    "        rag_points = self.extract_key_points(rag_answer)\n",
    "        \n",
    "        # Calculate key points matching\n",
    "        if gt_points and rag_points:\n",
    "            sim_matrix = self.calculate_similarity_matrix(gt_points, rag_points)\n",
    "            key_points_match = np.mean(np.max(sim_matrix, axis=1))\n",
    "        else:\n",
    "            key_points_match = 0.0\n",
    "        \n",
    "        # Extract numerical information\n",
    "        gt_numbers = self.extract_numerical_info(gt_answer)\n",
    "        rag_numbers = self.extract_numerical_info(rag_answer)\n",
    "        \n",
    "        # Calculate factual accuracy based on numerical information\n",
    "        if gt_numbers and rag_numbers:\n",
    "            factual_matrix = self.calculate_similarity_matrix(gt_numbers, rag_numbers)\n",
    "            factual_accuracy = np.mean(np.max(factual_matrix, axis=1))\n",
    "        else:\n",
    "            factual_accuracy = 1.0 if not gt_numbers else 0.0\n",
    "        \n",
    "        # Calculate overall answer similarity\n",
    "        answer_similarity = self._text_similarity(gt_answer, rag_answer)\n",
    "        \n",
    "        # Calculate completeness\n",
    "        completeness = len(rag_points) / len(gt_points) if gt_points else 1.0\n",
    "        \n",
    "        # Determine category\n",
    "        category = self._categorize_question(question)\n",
    "        \n",
    "        return EvaluationMetrics(\n",
    "            answer_similarity=answer_similarity,\n",
    "            factual_accuracy=factual_accuracy,\n",
    "            completeness=completeness,\n",
    "            key_points_match=key_points_match,\n",
    "            category=category\n",
    "        )\n",
    "    \n",
    "    def _categorize_question(self, question: str) -> str:\n",
    "        \"\"\"Categorize the question based on content\"\"\"\n",
    "        question = question.lower()\n",
    "        for category, keywords in self.categories.items():\n",
    "            if any(keyword in question for keyword in keywords):\n",
    "                return category\n",
    "        return 'other'\n",
    "    \n",
    "    def evaluate_system(self, \n",
    "                       ground_truth: Dict, \n",
    "                       rag_outputs: Dict) -> Dict:\n",
    "        \"\"\"Evaluate the entire RAG system\"\"\"\n",
    "        results = {\n",
    "            'overall_metrics': defaultdict(list),\n",
    "            'category_metrics': defaultdict(lambda: defaultdict(list)),\n",
    "            'error_analysis': defaultdict(list),\n",
    "            'details': []\n",
    "        }\n",
    "        \n",
    "        for qid, gt_item in ground_truth.items():\n",
    "            if qid not in rag_outputs:\n",
    "                logging.warning(f\"Missing RAG output for question {qid}\")\n",
    "                continue\n",
    "                \n",
    "            rag_item = rag_outputs[qid]\n",
    "            \n",
    "            try:\n",
    "                # Evaluate answer\n",
    "                metrics = self.evaluate_answer(\n",
    "                    gt_item['question'],\n",
    "                    gt_item['answer_data']['answer'],\n",
    "                    rag_item['answer_data']['answer']\n",
    "                )\n",
    "                \n",
    "                # Store results\n",
    "                results['overall_metrics']['answer_similarity'].append(metrics.answer_similarity)\n",
    "                results['overall_metrics']['factual_accuracy'].append(metrics.factual_accuracy)\n",
    "                results['overall_metrics']['completeness'].append(metrics.completeness)\n",
    "                results['overall_metrics']['key_points_match'].append(metrics.key_points_match)\n",
    "                \n",
    "                # Store category results\n",
    "                for metric_name, value in metrics.__dict__.items():\n",
    "                    if metric_name != 'category':\n",
    "                        results['category_metrics'][metrics.category][metric_name].append(value)\n",
    "                \n",
    "                # Error analysis\n",
    "                if metrics.answer_similarity < 0.7:\n",
    "                    results['error_analysis']['low_similarity'].append(qid)\n",
    "                if metrics.factual_accuracy < 0.8:\n",
    "                    results['error_analysis']['factual_errors'].append(qid)\n",
    "                if metrics.completeness < 0.7:\n",
    "                    results['error_analysis']['incomplete_answers'].append(qid)\n",
    "                \n",
    "                # Store detailed results\n",
    "                results['details'].append({\n",
    "                    'question_id': qid,\n",
    "                    'question': gt_item['question'],\n",
    "                    'metrics': metrics.__dict__\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error evaluating question {qid}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate summary metrics\n",
    "        results['summary'] = self._calculate_summary_metrics(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _calculate_summary_metrics(self, results: Dict) -> Dict:\n",
    "        \"\"\"Calculate summary metrics from detailed results\"\"\"\n",
    "        summary = {\n",
    "            'overall': {\n",
    "                metric: float(np.mean(values))\n",
    "                for metric, values in results['overall_metrics'].items()\n",
    "            },\n",
    "            'by_category': {\n",
    "                category: {\n",
    "                    metric: float(np.mean(values))\n",
    "                    for metric, values in metrics.items()\n",
    "                }\n",
    "                for category, metrics in results['category_metrics'].items()\n",
    "            },\n",
    "            'error_counts': {\n",
    "                error_type: len(questions)\n",
    "                for error_type, questions in results['error_analysis'].items()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def generate_report(self, results: Dict, output_path: Optional[str] = None) -> str:\n",
    "        \"\"\"Generate evaluation report\"\"\"\n",
    "        report = []\n",
    "        report.append(\"RAG System Evaluation Report\")\n",
    "        report.append(\"==========================\\n\")\n",
    "        \n",
    "        # Overall metrics\n",
    "        report.append(\"Overall Performance:\")\n",
    "        report.append(\"-----------------\")\n",
    "        for metric, value in results['summary']['overall'].items():\n",
    "            report.append(f\"{metric}: {value:.3f}\")\n",
    "        \n",
    "        # Category performance\n",
    "        report.append(\"\\nPerformance by Category:\")\n",
    "        report.append(\"----------------------\")\n",
    "        for category, metrics in results['summary']['by_category'].items():\n",
    "            report.append(f\"\\n{category.capitalize()}:\")\n",
    "            for metric, value in metrics.items():\n",
    "                report.append(f\"  {metric}: {value:.3f}\")\n",
    "        \n",
    "        # Error analysis\n",
    "        report.append(\"\\nError Analysis:\")\n",
    "        report.append(\"--------------\")\n",
    "        for error_type, count in results['summary']['error_counts'].items():\n",
    "            report.append(f\"{error_type}: {count} instances\")\n",
    "        \n",
    "        report_text = \"\\n\".join(report)\n",
    "        \n",
    "        if output_path:\n",
    "            with open(output_path, 'w') as f:\n",
    "                f.write(report_text)\n",
    "            logging.info(f\"Report saved to {output_path}\")\n",
    "        \n",
    "        return report_text\n",
    "\n",
    "def evaluate_rag_system(ground_truth_path: str, \n",
    "                       rag_output_path: str, \n",
    "                       report_path: Optional[str] = None) -> Tuple[Dict, str]:\n",
    "    \"\"\"Convenience function to run evaluation\"\"\"\n",
    "    evaluator = RobustRAGEvaluator()\n",
    "    \n",
    "    # Load data\n",
    "    with open(ground_truth_path, 'r') as f:\n",
    "        ground_truth = json.load(f)['ground_truth']\n",
    "    \n",
    "    with open(rag_output_path, 'r') as f:\n",
    "        rag_outputs = json.load(f)\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluator.evaluate_system(ground_truth, rag_outputs)\n",
    "    \n",
    "    # Generate report\n",
    "    report = evaluator.generate_report(results, report_path)\n",
    "    \n",
    "    return results, report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results, report = evaluate_rag_system(\n",
    "        ground_truth_path=\"data/ground_truth_2000.json\",\n",
    "        rag_output_path=\"rag_outputs.json\",\n",
    "        report_path=\"evaluation_report.txt\"\n",
    "    )\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704bd977-edd3-4c70-9be0-0ceeb8d7595b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
