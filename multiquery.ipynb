{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bf15072-5f8a-4a00-bc8e-3d655fae7244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredURLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cdf1d65-6e54-4812-bcea-d27b71b0a0e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('unique_links_list.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    items = [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ad39435-1e5c-42ae-9ecf-293f69ca507f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "809e5d27-b53d-4d2d-94f7-db1fa6cf3003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = UnstructuredURLLoader(urls=urls)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "505764e2-5d4c-412a-84ea-9465600cbabf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://datascience.uchicago.edu/education/masters-programs/ms-in-applied-data-science/'}, page_content=\"Elevate Your Expertise in Data Science\\n\\nThe University of Chicago’s MS in Applied Data Science program equips you with in-demand expertise and an unparalleled network of global alumni. Take the next step and start your application today.\\n\\nHow to Apply\\n\\nPrograms\\n\\nChoose from full- and part-time options in our In-Person and Online programs. Apply today!\\n\\nIn-Person Program\\n\\nIf you are an early career professional, need to complete a master's in one year, or require OPT STEM eligibility, the In-Person program is for you.\\n\\nLearn More\\n\\nOnline Program\\n\\nIf you want 360° flexibility and the same rigorous curriculum and outcomes as an in-person degree, the Online Program is for you.\\n\\nLearn More\\n\\nMBA/MS Program\\n\\nThe joint degree with UChicago’s Booth School of Business is ideal for ambitious students looking to supplement their MBA studies with a cutting-edge education in data science.\\n\\nLearn More\\n\\nApply Today!\\n\\nThe application for entry is open! Part-time Online Students: Apply by January 9, 2025, for a Spring 2025 start. All other Online and In-Person students can apply by June 23, 2025, for full- and part-time study for Autumn 2025 entry. Start your application here.\\n\\nHow to Apply\\n\\nYou have questions; we have answers. Whether you are just beginning your search or are ready to apply, we are here to help you take the next step.\\n\\nHow to Apply\\n\\nIndustry Leading Faculty\\n\\nUChicago’s Master’s in Applied Data Science program recruits industry leaders from across the country. Meet our instructors and learn how they are shaping the landscape of data science.\\n\\nLearn More\\n\\nData in Action - Capstone Projects\\n\\nWhether you are early in your career or more advanced, you will benefit from our real-world Capstone Experience. You will have the opportunity to help top companies across multiple sectors to solve real business problems. Sample Capstone Projects.\\n\\nLearn More\\n\\nStart Your Application\\n\\nThe application for entrance in Autumn 2025 is open! The final application deadline for Spring 2025 entrance is January 9, 2025, and the final application deadline for full- and part-time entrance in Autumn 2025 is June 23, 2025.\\n\\nThe In-Person program admits full- and part-time students for entrance in the autumn quarter annually. The Online program admits full- and part-time students for entrance in the autumn quarter. Online program part-time students may also begin the program in the spring quarter.\\n\\nStart your application\\n\\nRelated News, Insights, and Past Events\\n\\nBlogOct 31, 2024 What’s Game-Theoretic About Statistics? A Game-Theoretic Notion of Evidence and Cooperative Skepticism\\n\\nDSI NewsOct 29, 2024 Supporting Indigenous Data Sovereignty and Voter Education with North Dakota Native Vote\\n\\nDSI NewsOct 28, 2024 Exploring the Power of Inquiry at Chicago Data Night\\n\\nNov12Past\\xa0EventNov 12, 2024 Data Science Seminar Series\\n\\nApplied Data ScienceOct 22, 2024 Applying to the MS in Applied Data Science Program? Here’s What We Look For\\n\\nOct25Past\\xa0EventOct 25, 2024 Data, Ethics, and Policy Conference 2024: Governance in a Digital World\\n\\nApplied Data ScienceOct 16, 2024 The MS in Applied Data Science Program Celebrates 10 Years\\n\\nCampus NewsOct 15, 2024 UChicago Researchers Demonstrate the Quantifiable Uniqueness of Former President Donald Trump’s Language Use\\n\\nOct22Past\\xa0EventOct 22, 2024 Heather Whitney (UChicago): AI+Science Schmidt Fellows Speaker Series\\n\\nDSI NewsOct 07, 2024 Bringing Transparency to Carbon Credit Projects\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34b290f9-18ad-4374-9748-2e4e4ac7fa6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4069b2-1c64-49d4-89b9-1f7f6a2d6ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "970f27a8-bb82-4e55-9c7e-a60dcdf3e5d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1899"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5575ca7-b4c5-4913-9cd2-4499432a7423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = ''\n",
    "os.environ['OPENAI_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ac3763a-e6c6-4e49-bf29-0db5edc22a41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from re import search\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e25709bb-91ad-4780-872e-f63f32d0e92f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits,\n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"What are the Core Courses of the Applied Data Science Program?\")\n",
    "\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f08adbcb-369f-4678-b432-cb2e4d5708e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Given the context below, please provide an accurate and detailed response to the user\\'s inquiry about the MS in Applied Data Science program at the University of Chicago.\\nRespond with a JSON object (not in a code block) in the following format:\\n{{\\n    \"answer\": \"detailed answer using only information from the context\",\\n    \"confidence\": \"high/medium/low, based on how thoroughly the context supports your answer\",\\n    \"reasoning\": \"concise explanation of how the context informs your answer\"\\n}}\\n\\nIf the context does not contain enough information, respond with:\\n{{\\n    \"answer\": \"I\\'m sorry, but I cannot answer this question based on the provided context.\",\\n    \"confidence\": \"low\",\\n    \"reasoning\": \"The provided context lacks sufficient details to answer this inquiry\"\\n}}\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nGuidelines:\\n1. Use only information from the context provided.\\n2. Craft a detailed response that addresses the inquiry directly.\\n3. Use bullet points for distinct points if applicable.\\n4. Include a relevant URL if it directly supports the answer.\\n5. Assign confidence based on the relevance and completeness of the context.\\n6. Briefly explain your reasoning, focusing on how the context justifies your answer.\\n7. Return ONLY the JSON object without any code block markers or extraneous text.\\n'))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Given the context below, please provide an accurate and detailed response to the user's inquiry about the MS in Applied Data Science program at the University of Chicago.\n",
    "Respond with a JSON object (not in a code block) in the following format:\n",
    "{{\n",
    "    \"answer\": \"detailed answer using only information from the context\",\n",
    "    \"confidence\": \"high/medium/low, based on how thoroughly the context supports your answer\",\n",
    "    \"reasoning\": \"concise explanation of how the context informs your answer\"\n",
    "}}\n",
    "\n",
    "If the context does not contain enough information, respond with:\n",
    "{{\n",
    "    \"answer\": \"I'm sorry, but I cannot answer this question based on the provided context.\",\n",
    "    \"confidence\": \"low\",\n",
    "    \"reasoning\": \"The provided context lacks sufficient details to answer this inquiry\"\n",
    "}}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Guidelines:\n",
    "1. Use only information from the context provided.\n",
    "2. Craft a detailed response that addresses the inquiry directly.\n",
    "3. Use bullet points for distinct points if applicable.\n",
    "4. Include a relevant URL if it directly supports the answer.\n",
    "5. Assign confidence based on the relevance and completeness of the context.\n",
    "6. Briefly explain your reasoning, focusing on how the context justifies your answer.\n",
    "7. Return ONLY the JSON object without any code block markers or extraneous text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4967df19-e254-47fe-aff3-e0db4f42f168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3e9eb69-e0ad-48b1-bda6-41ea30e04a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. Multi-Query Generation\n",
    "template_multiquery = \"\"\"You are an AI language model assistant. Your task is to generate \n",
    "five different versions of the given user question to retrieve relevant documents\n",
    "from a vector database. Generate diverse perspectives on the user question to help\n",
    "overcome some limitations of distance-based similarity search.\n",
    "Provide these alternative questions separated by newlines.\n",
    "\n",
    "Original question: {question}\"\"\"\n",
    "\n",
    "prompt_multiquery = ChatPromptTemplate.from_template(template_multiquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d33570d-782d-4885-a5de-8f498234c86d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. RAG-Fusion Generation\n",
    "template_ragfusion = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "\n",
    "prompt_ragfusion = ChatPromptTemplate.from_template(template_ragfusion)\n",
    "\n",
    "# Generate queries chains\n",
    "generate_queries_multiquery = (\n",
    "    prompt_multiquery \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "generate_queries_ragfusion = (\n",
    "    prompt_ragfusion \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# Function for unique union and rank fusion\n",
    "def get_unique_union(documents):\n",
    "    \"\"\"Get unique union of retrieved documents\"\"\"\n",
    "    seen = set()\n",
    "    unique_docs = []\n",
    "    \n",
    "    # Flatten list of lists\n",
    "    flattened_docs = [doc for sublist in documents for doc in sublist]\n",
    "    \n",
    "    for doc in flattened_docs:\n",
    "        doc_str = f\"{doc.page_content}{doc.metadata}\"\n",
    "        if doc_str not in seen:\n",
    "            seen.add(doc_str)\n",
    "            unique_docs.append(doc)\n",
    "            \n",
    "    return unique_docs\n",
    "\n",
    "def reciprocal_rank_fusion(results, k=60):\n",
    "    \"\"\"Reciprocal Rank Fusion for combining multiple ranked lists\"\"\"\n",
    "    flattened_docs = [doc for sublist in results for doc in sublist]\n",
    "    fused_scores = {}\n",
    "    \n",
    "    for rank, doc in enumerate(flattened_docs):\n",
    "        doc_str = f\"{doc.page_content}{doc.metadata}\"\n",
    "        if doc_str not in fused_scores:\n",
    "            fused_scores[doc_str] = 0\n",
    "        fused_scores[doc_str] += 1 / (rank + k)\n",
    "    \n",
    "    reranked_docs = []\n",
    "    for doc_str, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "        for doc in flattened_docs:\n",
    "            if f\"{doc.page_content}{doc.metadata}\" == doc_str:\n",
    "                reranked_docs.append((doc, score))\n",
    "                break\n",
    "                \n",
    "    return reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "846a53a8-0282-4840-be7e-fd3febdb1459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create retrieval chains\n",
    "retrieval_chain_multiquery = (\n",
    "    generate_queries_multiquery \n",
    "    | retriever.map() \n",
    "    | get_unique_union\n",
    ")\n",
    "\n",
    "retrieval_chain_ragfusion = (\n",
    "    generate_queries_ragfusion \n",
    "    | retriever.map() \n",
    "    | reciprocal_rank_fusion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f99745c8-0d74-49fc-9dd7-a9225ca341ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is tuition cost for the program?\n",
      "\n",
      "Multi-Query Approach:\n",
      "Generated queries:\n",
      "- 1. How much does the program charge for tuition fees?\n",
      "- 2. Can you provide information on the program's tuition expenses?\n",
      "- 3. What are the costs associated with enrolling in the program?\n",
      "- 4. How expensive is it to attend the program in terms of tuition?\n",
      "- 5. What is the price of tuition for the program?\n",
      "\n",
      "Top retrieved chunks:\n",
      "- MS in Applied Data Science\n",
      "\n",
      "Overview\n",
      "\n",
      "In-Person Program\n",
      "\n",
      "Online Program\n",
      "\n",
      "Capstone Projects\n",
      "\n",
      "Course Progressions\n",
      "\n",
      "How to Apply\n",
      "\n",
      "Events & Deadlines\n",
      "\n",
      "Tuition, Fees, & Aid\n",
      "\n",
      "Our Students\n",
      "\n",
      "Faculty, Instruct...\n",
      "- Other Scholarships\n",
      "\n",
      "Students are encouraged to investigate scholarships offered through various civic and professional organizations, foundations and state agencies. One place to search for scholarshi...\n",
      "- Elective Courses (4)\n",
      "\n",
      "You will complete 4 required electives toward your 12-course degree program. We continually add electives to evolve with the data science landscape. Past electives include Advanc...\n",
      "\n",
      "Answer:\n",
      "{\n",
      "    \"answer\": \"The tuition cost for the MS in Applied Data Science program at the University of Chicago is $5,967 per course, with a total tuition of $71,604. Additionally, there is a non-refundable program enrollment deposit of $1,500, which is credited toward your first quarter’s tuition balance. It is important to note that tuition is expected to increase by 3-7% per year.\",\n",
      "    \"confidence\": \"high\",\n",
      "    \"reasoning\": \"The context provides specific details about the tuition cost per course and the total tuition for the program, as well as information about the enrollment deposit and expected annual tuition increases.\"\n",
      "}\n",
      "\n",
      "RAG-Fusion Approach:\n",
      "Generated queries:\n",
      "- 1. Average tuition cost for the program\n",
      "- 2. Tuition fees for the program\n",
      "- 3. Cost of attendance for the program\n",
      "- 4. Scholarships available for the program\n",
      "\n",
      "Top retrieved chunks (with scores):\n",
      "- Score 0.0897: Other Scholarships\n",
      "\n",
      "Students are encouraged to investigate scholarships offered through various civic and professional organizations, foundations and state agencies. One place to search for scholarshi...\n",
      "- Score 0.0492: MS in Applied Data Science\n",
      "\n",
      "Overview\n",
      "\n",
      "In-Person Program\n",
      "\n",
      "Online Program\n",
      "\n",
      "Capstone Projects\n",
      "\n",
      "Course Progressions\n",
      "\n",
      "How to Apply\n",
      "\n",
      "Events & Deadlines\n",
      "\n",
      "Tuition, Fees, & Aid\n",
      "\n",
      "Our Students\n",
      "\n",
      "Faculty, Instruct...\n",
      "- Score 0.0448: Elective Courses (4)\n",
      "\n",
      "You will complete 4 required electives toward your 12-course degree program. We continually add electives to evolve with the data science landscape. Past electives include Advanc...\n",
      "\n",
      "Answer:\n",
      "{\n",
      "    \"answer\": \"The tuition cost for the MS in Applied Data Science program at the University of Chicago is $5,967 per course, with a total tuition of $71,604. Additionally, there is a non-refundable program enrollment deposit of $1,500, which is credited toward your first quarter’s tuition balance. It is important to note that tuition is expected to increase by 3-7% per year.\",\n",
      "    \"confidence\": \"high\",\n",
      "    \"reasoning\": \"The context provides specific details about the tuition cost per course and the total tuition for the program, as well as information about the enrollment deposit and expected annual tuition increases. This directly answers the user's inquiry about the tuition cost.\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Question: What scholarships are available for the program?\n",
      "\n",
      "Multi-Query Approach:\n",
      "Generated queries:\n",
      "- 1. Can you provide information on scholarships offered for this program?\n",
      "- 2. Are there any financial aid options specifically for students in this program?\n",
      "- 3. Which scholarships can students in this program apply for?\n",
      "- 4. What are the funding opportunities available for participants of this program?\n",
      "- 5. Are there any grants or scholarships tailored to students enrolled in this particular program?\n",
      "\n",
      "Top retrieved chunks:\n",
      "- Your Time, Your Advancement\n",
      "\n",
      "You will engage in weekly, synchronous (real-time) classes with your instructors and peers. Live lectures provide the right forum to ask questions, work with peers, and ma...\n",
      "- Other Scholarships\n",
      "\n",
      "Students are encouraged to investigate scholarships offered through various civic and professional organizations, foundations and state agencies. One place to search for scholarshi...\n",
      "- Elective Courses (4)\n",
      "\n",
      "You will complete 4 required electives toward your 12-course degree program. We continually add electives to evolve with the data science landscape. Past electives include Advanc...\n",
      "\n",
      "Answer:\n",
      "{\n",
      "    \"answer\": \"Merit scholarships are available for eligible applicants to the MS in Applied Data Science program at the University of Chicago. Once you apply to the program, you will be automatically considered for a scholarship. Early applications are highly encouraged. Additionally, students are encouraged to investigate scholarships offered through various civic and professional organizations, foundations, and state agencies. One place to search for scholarships is the financial aid information web page sponsored by the National Association of Student Aid Administration.\",\n",
      "    \"confidence\": \"high\",\n",
      "    \"reasoning\": \"The context provides specific information about the availability of merit scholarships for eligible applicants and encourages early application. It also suggests exploring external scholarships through civic and professional organizations, which directly addresses the inquiry about available scholarships.\"\n",
      "}\n",
      "\n",
      "RAG-Fusion Approach:\n",
      "Generated queries:\n",
      "- 1. \"Scholarships for [specific program name]\"\n",
      "- 2. \"Financial aid options for [specific program name]\"\n",
      "- 3. \"Merit-based scholarships for [specific program name]\"\n",
      "- 4. \"Need-based scholarships for [specific program name]\"\n",
      "\n",
      "Top retrieved chunks (with scores):\n",
      "- Score 0.0940: Other Scholarships\n",
      "\n",
      "Students are encouraged to investigate scholarships offered through various civic and professional organizations, foundations and state agencies. One place to search for scholarshi...\n",
      "- Score 0.0469: of sample electives (subject to change) appear at the bottom of the In-Person and Online Program pages.Quarter 5 • 10 Weeks  Capstone  Data Science Capstone Project Letter Grade The required Capstone ...\n",
      "- Score 0.0429: Elective Courses (4)\n",
      "\n",
      "You will complete 4 required electives toward your 12-course degree program. We continually add electives to evolve with the data science landscape. Past electives include Advanc...\n",
      "\n",
      "Answer:\n",
      "{\n",
      "    \"answer\": \"Merit scholarships are available for eligible applicants to the MS in Applied Data Science program at the University of Chicago. Once you apply to the program, you will be automatically considered for a scholarship. Early applications are highly encouraged. Additionally, students are encouraged to investigate scholarships offered through various civic and professional organizations, foundations, and state agencies. For more information on financial aid, including scholarships, please visit the Graduate Aid page.\",\n",
      "    \"confidence\": \"high\",\n",
      "    \"reasoning\": \"The context provides specific information about the availability of merit scholarships for eligible applicants and the automatic consideration upon application. It also suggests exploring external scholarships and provides resources for further financial aid information.\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Question: What are the minimum scores for the TOEFL and IELTS English Language Requirement?\n",
      "\n",
      "Multi-Query Approach:\n",
      "Generated queries:\n",
      "- 1. What are the lowest acceptable scores for meeting the English Language Requirement for TOEFL and IELTS?\n",
      "- 2. Can you provide information on the minimum TOEFL and IELTS scores needed to fulfill the English Language Requirement?\n",
      "- 3. What are the cutoff scores for the TOEFL and IELTS exams in order to satisfy the English Language Requirement?\n",
      "- 4. What are the minimum scores required for TOEFL and IELTS to meet the English Language Requirement criteria?\n",
      "- 5. Could you please specify the minimum TOEFL and IELTS scores that are considered acceptable for the English Language Requirement?\n",
      "\n",
      "Top retrieved chunks:\n",
      "- schools and divisions of the University of Chicago who do not meet the criteria above must submit proof of English Language Proficiency. This policy applies to all graduate programs; the score level r...\n",
      "\n",
      "Answer:\n",
      "{\n",
      "    \"answer\": \"The minimum required score for the TOEFL is 102 overall. The minimum required score for the IELTS is an overall score of seven. Note that students are required to take the Academic Reading/Writing test within IELTS, not the General Training Reading/Writing test.\",\n",
      "    \"confidence\": \"high\",\n",
      "    \"reasoning\": \"The context provides specific details about the minimum scores required for both the TOEFL and IELTS tests, including the type of IELTS test that is acceptable. This directly answers the user's inquiry about the English Language Requirement scores.\"\n",
      "}\n",
      "\n",
      "RAG-Fusion Approach:\n",
      "Generated queries:\n",
      "- 1. What are the minimum TOEFL scores required for English language proficiency?\n",
      "- 2. What are the minimum IELTS scores needed to meet English language requirements?\n",
      "- 3. TOEFL and IELTS minimum score comparison for English language proficiency.\n",
      "- 4. Universities with the highest TOEFL and IELTS score requirements for admission.\n",
      "\n",
      "Top retrieved chunks (with scores):\n",
      "- Score 0.1837: schools and divisions of the University of Chicago who do not meet the criteria above must submit proof of English Language Proficiency. This policy applies to all graduate programs; the score level r...\n",
      "\n",
      "Answer:\n",
      "{\n",
      "    \"answer\": \"The minimum required score for the TOEFL is 102 overall. For the IELTS, the minimum required score is an overall score of seven. Note that students are required to take the Academic Reading/Writing test within IELTS, not the General Training Reading/Writing test.\",\n",
      "    \"confidence\": \"high\",\n",
      "    \"reasoning\": \"The context provides specific details about the minimum scores required for both the TOEFL and IELTS tests, including the type of IELTS test that must be taken. This directly answers the user's inquiry about the English Language Requirement scores.\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Question: Is there an application fee waiver?\n",
      "\n",
      "Multi-Query Approach:\n",
      "Generated queries:\n",
      "- 1. What are the criteria for qualifying for an application fee waiver?\n",
      "- 2. How can I request an exemption from paying the application fee?\n",
      "- 3. Are there any circumstances under which the application fee is waived?\n",
      "- 4. Can I be eligible for a waiver of the application fee?\n",
      "- 5. What options are available for those unable to pay the application fee?\n",
      "\n",
      "Top retrieved chunks:\n",
      "- applieddatascience-admissions@uchicago.edu.\n",
      "\n",
      "If your institution cannot send your documents electronically, please have them send your transcripts to the following mailing address.\n",
      "\n",
      "Please note, if do...\n",
      "- Other Scholarships\n",
      "\n",
      "Students are encouraged to investigate scholarships offered through various civic and professional organizations, foundations and state agencies. One place to search for scholarshi...\n",
      "\n",
      "Answer:\n",
      "{\n",
      "    \"answer\": \"For questions regarding an application fee waiver, please refer to the Physical Sciences Division fee waiver policy.\",\n",
      "    \"confidence\": \"medium\",\n",
      "    \"reasoning\": \"The context mentions the application fee and directs inquiries about fee waivers to the Physical Sciences Division fee waiver policy, but it does not provide specific details about the waiver process or eligibility.\"\n",
      "}\n",
      "\n",
      "RAG-Fusion Approach:\n",
      "Generated queries:\n",
      "- 1. How to request an application fee waiver?\n",
      "- 2. Which colleges offer application fee waivers?\n",
      "- 3. Are there any scholarships that include an application fee waiver?\n",
      "- 4. What are the eligibility criteria for an application fee waiver?\n",
      "\n",
      "Top retrieved chunks (with scores):\n",
      "- Score 0.1345: Other Scholarships\n",
      "\n",
      "Students are encouraged to investigate scholarships offered through various civic and professional organizations, foundations and state agencies. One place to search for scholarshi...\n",
      "- Score 0.0492: applieddatascience-admissions@uchicago.edu.\n",
      "\n",
      "If your institution cannot send your documents electronically, please have them send your transcripts to the following mailing address.\n",
      "\n",
      "Please note, if do...\n",
      "\n",
      "Answer:\n",
      "{\n",
      "    \"answer\": \"For questions regarding an application fee waiver, please refer to the Physical Sciences Division fee waiver policy.\",\n",
      "    \"confidence\": \"medium\",\n",
      "    \"reasoning\": \"The context mentions that questions about an application fee waiver should be directed to the Physical Sciences Division fee waiver policy, but it does not provide specific details about the conditions or availability of such waivers.\"\n",
      "}\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def test_retrieval_methods_detailed(questions, retrieval_chain_multiquery, retrieval_chain_ragfusion, prompt, llm):\n",
    "    \"\"\"Compare multiquery and RAG-fusion approaches with detailed output\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for question in questions:\n",
    "        # Generate queries for both methods\n",
    "        multiquery_queries = generate_queries_multiquery.invoke({\"question\": question})\n",
    "        ragfusion_queries = generate_queries_ragfusion.invoke({\"question\": question})\n",
    "        \n",
    "        # Get retrieved documents using both methods\n",
    "        multiquery_docs = retrieval_chain_multiquery.invoke({\"question\": question})\n",
    "        fusion_docs_with_scores = retrieval_chain_ragfusion.invoke({\"question\": question})\n",
    "        \n",
    "        # Extract just the documents from fusion results (without scores)\n",
    "        fusion_docs = [doc for doc, score in fusion_docs_with_scores]\n",
    "        \n",
    "        # Create RAG chain\n",
    "        rag_chain = prompt | llm | StrOutputParser()\n",
    "        \n",
    "        # Generate responses\n",
    "        multiquery_answer = rag_chain.invoke({\n",
    "            \"context\": \"\\n\\n\".join(doc.page_content for doc in multiquery_docs),\n",
    "            \"question\": question\n",
    "        })\n",
    "        \n",
    "        fusion_answer = rag_chain.invoke({\n",
    "            \"context\": \"\\n\\n\".join(doc.page_content for doc in fusion_docs[:4]),  # Using top 4 docs\n",
    "            \"question\": question\n",
    "        })\n",
    "        \n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"multiquery\": {\n",
    "                \"generated_queries\": multiquery_queries,\n",
    "                \"top_chunks\": [doc.page_content[:200] + \"...\" for doc in multiquery_docs[:3]],  # First 200 chars\n",
    "                \"answer\": multiquery_answer\n",
    "            },\n",
    "            \"ragfusion\": {\n",
    "                \"generated_queries\": ragfusion_queries,\n",
    "                \"top_chunks_with_scores\": [\n",
    "                    (doc.page_content[:200] + \"...\", score) \n",
    "                    for doc, score in fusion_docs_with_scores[:3]\n",
    "                ],\n",
    "                \"answer\": fusion_answer\n",
    "            }\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test questions\n",
    "Questions = [\n",
    "    \"What is tuition cost for the program?\",\n",
    "    \"What scholarships are available for the program?\",\n",
    "    \"What are the minimum scores for the TOEFL and IELTS English Language Requirement?\",\n",
    "    \"Is there an application fee waiver?\"\n",
    "]\n",
    "\n",
    "# Run detailed comparison\n",
    "detailed_results = test_retrieval_methods_detailed(\n",
    "    Questions,\n",
    "    retrieval_chain_multiquery,\n",
    "    retrieval_chain_ragfusion,\n",
    "    prompt,\n",
    "    llm\n",
    ")\n",
    "\n",
    "# Print detailed results\n",
    "for result in detailed_results:\n",
    "    print(f\"\\nQuestion: {result['question']}\")\n",
    "    \n",
    "    print(\"\\nMulti-Query Approach:\")\n",
    "    print(\"Generated queries:\")\n",
    "    for q in result['multiquery']['generated_queries']:\n",
    "        print(f\"- {q}\")\n",
    "    print(\"\\nTop retrieved chunks:\")\n",
    "    for chunk in result['multiquery']['top_chunks']:\n",
    "        print(f\"- {chunk}\")\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(result['multiquery']['answer'])\n",
    "    \n",
    "    print(\"\\nRAG-Fusion Approach:\")\n",
    "    print(\"Generated queries:\")\n",
    "    for q in result['ragfusion']['generated_queries']:\n",
    "        print(f\"- {q}\")\n",
    "    print(\"\\nTop retrieved chunks (with scores):\")\n",
    "    for chunk, score in result['ragfusion']['top_chunks_with_scores']:\n",
    "        print(f\"- Score {score:.4f}: {chunk}\")\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(result['ragfusion']['answer'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a3e174a-002a-4e7d-b087-1e03c183cd67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "@dataclass\n",
    "class RetrievalMetrics:\n",
    "    precision_at_k: float\n",
    "    recall_at_k: float\n",
    "    mean_reciprocal_rank: float\n",
    "    relevance_score: float\n",
    "    \n",
    "def calculate_retrieval_metrics(retrieved_docs: List, \n",
    "                              question: str,\n",
    "                              embeddings,\n",
    "                              k: int = 4) -> RetrievalMetrics:\n",
    "    \"\"\"Calculate retrieval metrics for a single question\"\"\"\n",
    "    \n",
    "    # 1. Calculate semantic similarity between question and retrieved docs\n",
    "    question_embedding = embeddings.embed_query(question)\n",
    "    \n",
    "    # Handle both regular docs and (doc, score) tuples\n",
    "    if isinstance(retrieved_docs[0], tuple):\n",
    "        doc_contents = [doc[0].page_content for doc in retrieved_docs[:k]]\n",
    "    else:\n",
    "        doc_contents = [doc.page_content for doc in retrieved_docs[:k]]\n",
    "    \n",
    "    doc_embeddings = [embeddings.embed_query(doc) for doc in doc_contents]\n",
    "    \n",
    "    similarities = [\n",
    "        cosine_similarity(\n",
    "            np.array(question_embedding).reshape(1, -1),\n",
    "            np.array(doc_embedding).reshape(1, -1)\n",
    "        )[0][0]\n",
    "        for doc_embedding in doc_embeddings\n",
    "    ]\n",
    "    \n",
    "    # 2. Calculate metrics\n",
    "    # Assume documents with similarity > 0.5 are relevant\n",
    "    relevant_docs = [sim > 0.5 for sim in similarities]\n",
    "    \n",
    "    # Precision@k\n",
    "    precision = sum(relevant_docs) / k if k > 0 else 0\n",
    "    \n",
    "    # Recall@k (assuming we know total relevant docs = 10 for example)\n",
    "    total_relevant = 10  # This should be adjusted based on your knowledge\n",
    "    recall = sum(relevant_docs) / total_relevant if total_relevant > 0 else 0\n",
    "    \n",
    "    # Mean Reciprocal Rank (MRR)\n",
    "    for i, is_relevant in enumerate(relevant_docs):\n",
    "        if is_relevant:\n",
    "            mrr = 1 / (i + 1)\n",
    "            break\n",
    "    else:\n",
    "        mrr = 0\n",
    "        \n",
    "    # Average similarity score\n",
    "    avg_similarity = np.mean(similarities)\n",
    "    \n",
    "    return RetrievalMetrics(\n",
    "        precision_at_k=precision,\n",
    "        recall_at_k=recall,\n",
    "        mean_reciprocal_rank=mrr,\n",
    "        relevance_score=avg_similarity\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b269577d-5952-4ebd-8740-b9bfee629fea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is tuition cost for the program?\n",
      "\n",
      "Regular RAG Metrics:\n",
      "Precision@4: 0.750\n",
      "Recall@4: 0.300\n",
      "MRR: 1.000\n",
      "Avg Relevance: 0.834\n",
      "\n",
      "Answer: {\n",
      "    \"answer\": \"The tuition for the MS in Applied Data Science program at the University of Chicago is $5,967 per course, with a total tuition cost of $71,604. Additionally, there is a non-refundable program enrollment deposit of $1,500, which is credited toward the first quarter’s tuition balance. It is important to note that tuition is expected to increase by 3-7% per year. The program also offers merit-based scholarships, and partial tuition scholarships are available to top applicants without requiring a separate application. Candidates are encouraged to apply early to maximize their chances of securing a scholarship.\",\n",
      "    \"confidence\": \"high\",\n",
      "    \"reasoning\": \"The context provides specific details about the tuition cost per course and the total tuition for the program, as well as information about the enrollment deposit and potential tuition increases. It also mentions the availability of scholarships, which directly addresses the user's inquiry about the tuition cost.\"\n",
      "}\n",
      "\n",
      "Multi-Query Metrics:\n",
      "Precision@4: 0.750\n",
      "Recall@4: 0.300\n",
      "MRR: 1.000\n",
      "Avg Relevance: 0.813\n",
      "\n",
      "Answer: {\n",
      "    \"answer\": \"The tuition cost for the MS in Applied Data Science program at the University of Chicago is $5,967 per course, with a total tuition of $71,604. Additionally, there is a non-refundable program enrollment deposit of $1,500, which is credited toward your first quarter’s tuition balance. It is important to note that tuition is expected to increase by 3-7% per year. The program also offers merit-based scholarships to top applicants, which do not require a separate application. Early application submission is recommended to maximize the chances of securing a scholarship.\",\n",
      "    \"confidence\": \"high\",\n",
      "    \"reasoning\": \"The context provides specific details about the tuition cost per course and the total tuition for the program, as well as information about the enrollment deposit and potential tuition increases. It also mentions the availability of merit-based scholarships, making the information comprehensive and directly relevant to the inquiry.\"\n",
      "}\n",
      "\n",
      "RAG-Fusion Metrics:\n",
      "Precision@4: 0.500\n",
      "Recall@4: 0.200\n",
      "MRR: 1.000\n",
      "Avg Relevance: 0.812\n",
      "\n",
      "Answer: {\n",
      "    \"answer\": \"The tuition cost for the MS in Applied Data Science program at the University of Chicago is $5,967 per course, with a total tuition of $71,604. Additionally, there is a non-refundable program enrollment deposit of $1,500, which is credited toward the first quarter’s tuition balance. It is important to note that tuition is expected to increase by 3-7% per year. The program also offers merit-based scholarships to top applicants, which do not require a separate application.\",\n",
      "    \"confidence\": \"high\",\n",
      "    \"reasoning\": \"The context provides specific details about the tuition cost per course and the total tuition for the program, as well as information about the enrollment deposit and potential tuition increases. It also mentions the availability of scholarships, which directly addresses the user's inquiry about the tuition cost.\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Question: What scholarships are available for the program?\n",
      "\n",
      "Regular RAG Metrics:\n",
      "Precision@4: 0.750\n",
      "Recall@4: 0.300\n",
      "MRR: 1.000\n",
      "Avg Relevance: 0.850\n",
      "\n",
      "Answer: {\n",
      "    \"answer\": \"The MS in Applied Data Science program at the University of Chicago offers merit scholarships for eligible applicants. Once you apply to the program, you will be automatically considered for a scholarship, and early applications are highly encouraged. Additionally, students are encouraged to investigate scholarships offered through various civic and professional organizations, foundations, and state agencies. A useful resource for searching scholarships is the financial aid information web page sponsored by the National Association of Student Aid Administration.\",\n",
      "    \"confidence\": \"high\",\n",
      "    \"reasoning\": \"The context provides specific information about the availability of merit scholarships for eligible applicants and the automatic consideration upon application. It also mentions the encouragement to explore external scholarships and provides a resource for finding them, which directly answers the question about available scholarships.\"\n",
      "}\n",
      "\n",
      "Multi-Query Metrics:\n",
      "Precision@4: 0.750\n",
      "Recall@4: 0.300\n",
      "MRR: 1.000\n",
      "Avg Relevance: 0.794\n",
      "\n",
      "Answer: {\n",
      "    \"answer\": \"Merit scholarships are available for eligible applicants to the MS in Applied Data Science program at the University of Chicago. Once you apply to the program, you will be automatically considered for a scholarship. Early applications are highly encouraged. Additionally, students are encouraged to investigate scholarships offered through various civic and professional organizations, foundations, and state agencies. One place to search for scholarships is the financial aid information web page sponsored by the National Association of Student Aid Administration.\",\n",
      "    \"confidence\": \"high\",\n",
      "    \"reasoning\": \"The context provides specific information about the availability of merit scholarships for eligible applicants and encourages early application. It also suggests exploring external scholarships through civic and professional organizations, foundations, and state agencies, indicating a comprehensive approach to financial aid opportunities.\"\n",
      "}\n",
      "\n",
      "RAG-Fusion Metrics:\n",
      "Precision@4: 0.750\n",
      "Recall@4: 0.300\n",
      "MRR: 1.000\n",
      "Avg Relevance: 0.792\n",
      "\n",
      "Answer: {\n",
      "    \"answer\": \"Merit scholarships are available for eligible applicants to the MS in Applied Data Science program at the University of Chicago. Once you apply to the program, you will be automatically considered for a scholarship. Early applications are highly encouraged. Additionally, students are encouraged to investigate scholarships offered through various civic and professional organizations, foundations, and state agencies. For more information on financial aid, including scholarships, please visit the Graduate Aid page.\",\n",
      "    \"confidence\": \"high\",\n",
      "    \"reasoning\": \"The context provides specific information about the availability of merit scholarships for eligible applicants and encourages early application. It also suggests exploring external scholarships and provides resources for further financial aid information, which directly addresses the inquiry about scholarships.\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Question: What are the minimum scores for the TOEFL and IELTS English Language Requirement?\n",
      "\n",
      "Regular RAG Metrics:\n",
      "Precision@4: 0.750\n",
      "Recall@4: 0.300\n",
      "MRR: 1.000\n",
      "Avg Relevance: 0.885\n",
      "\n",
      "Answer: {\n",
      "    \"answer\": \"The minimum required score for the TOEFL is 102 overall. The minimum required score for the IELTS is an overall score of seven. Note that students are required to take the Academic Reading/Writing test within IELTS, not the General Training Reading/Writing test.\",\n",
      "    \"confidence\": \"high\",\n",
      "    \"reasoning\": \"The context explicitly states the minimum required scores for both the TOEFL and IELTS tests, including specific details about the type of IELTS test required.\"\n",
      "}\n",
      "\n",
      "Multi-Query Metrics:\n",
      "Precision@4: 0.250\n",
      "Recall@4: 0.100\n",
      "MRR: 1.000\n",
      "Avg Relevance: 0.885\n",
      "\n",
      "Answer: {\n",
      "    \"answer\": \"The minimum required score for the TOEFL is 102 overall. The minimum required score for the IELTS is an overall score of seven. Note that students are required to take the Academic Reading/Writing test within IELTS, not the General Training Reading/Writing test.\",\n",
      "    \"confidence\": \"high\",\n",
      "    \"reasoning\": \"The context provides specific details about the minimum scores required for both the TOEFL and IELTS tests, which directly answers the user's inquiry.\"\n",
      "}\n",
      "\n",
      "RAG-Fusion Metrics:\n",
      "Precision@4: 0.250\n",
      "Recall@4: 0.100\n",
      "MRR: 1.000\n",
      "Avg Relevance: 0.885\n",
      "\n",
      "Answer: {\n",
      "    \"answer\": \"The minimum required score for the TOEFL is 102 overall. The minimum required score for the IELTS is an overall score of seven. Note that students are required to take the Academic Reading/Writing test within IELTS, not the General Training Reading/Writing test.\",\n",
      "    \"confidence\": \"high\",\n",
      "    \"reasoning\": \"The context provides specific details about the minimum scores required for both the TOEFL and IELTS tests, which directly answers the user's inquiry.\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Question: Is there an application fee waiver?\n",
      "\n",
      "Regular RAG Metrics:\n",
      "Precision@4: 0.750\n",
      "Recall@4: 0.300\n",
      "MRR: 1.000\n",
      "Avg Relevance: 0.815\n",
      "\n",
      "Answer: {\n",
      "    \"answer\": \"I'm sorry, but I cannot answer this question based on the provided context.\",\n",
      "    \"confidence\": \"low\",\n",
      "    \"reasoning\": \"The provided context lacks sufficient details to answer this inquiry\"\n",
      "}\n",
      "\n",
      "Multi-Query Metrics:\n",
      "Precision@4: 0.500\n",
      "Recall@4: 0.200\n",
      "MRR: 1.000\n",
      "Avg Relevance: 0.816\n",
      "\n",
      "Answer: {\n",
      "    \"answer\": \"For questions regarding an application fee waiver, please refer to the Physical Sciences Division fee waiver policy.\",\n",
      "    \"confidence\": \"medium\",\n",
      "    \"reasoning\": \"The context mentions that questions about an application fee waiver should be directed to the Physical Sciences Division fee waiver policy, but it does not provide specific details about the waiver itself.\"\n",
      "}\n",
      "\n",
      "RAG-Fusion Metrics:\n",
      "Precision@4: 0.500\n",
      "Recall@4: 0.200\n",
      "MRR: 1.000\n",
      "Avg Relevance: 0.816\n",
      "\n",
      "Answer: {\n",
      "    \"answer\": \"For questions regarding an application fee waiver, please refer to the Physical Sciences Division fee waiver policy.\",\n",
      "    \"confidence\": \"medium\",\n",
      "    \"reasoning\": \"The context mentions that questions about an application fee waiver should be directed to the Physical Sciences Division fee waiver policy, but it does not provide specific details about the conditions or availability of such waivers.\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Average Metrics Across All Questions:\n",
      "\n",
      "Regular RAG:\n",
      "Avg Precision@4: 0.750\n",
      "Avg Recall@4: 0.300\n",
      "Avg MRR: 1.000\n",
      "Avg Relevance: 0.846\n",
      "\n",
      "Multiquery RAG:\n",
      "Avg Precision@4: 0.562\n",
      "Avg Recall@4: 0.225\n",
      "Avg MRR: 1.000\n",
      "Avg Relevance: 0.827\n",
      "\n",
      "Ragfusion RAG:\n",
      "Avg Precision@4: 0.500\n",
      "Avg Recall@4: 0.200\n",
      "Avg MRR: 1.000\n",
      "Avg Relevance: 0.826\n"
     ]
    }
   ],
   "source": [
    "# 1. Regular RAG Pipeline\n",
    "regular_retrieval_chain = retriever\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 2. Multi-Query (already implemented)\n",
    "retrieval_chain_multiquery = (\n",
    "    generate_queries_multiquery \n",
    "    | retriever.map() \n",
    "    | get_unique_union\n",
    ")\n",
    "\n",
    "# 3. RAG-Fusion (already implemented)\n",
    "retrieval_chain_ragfusion = (\n",
    "    generate_queries_ragfusion \n",
    "    | retriever.map() \n",
    "    | reciprocal_rank_fusion\n",
    ")\n",
    "\n",
    "def compare_three_retrieval_methods(questions: List[str], \n",
    "                                  regular_chain,\n",
    "                                  multiquery_chain, \n",
    "                                  ragfusion_chain,\n",
    "                                  embeddings):\n",
    "    \"\"\"Compare all three retrieval methods using multiple metrics\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    for question in questions:\n",
    "        # Get retrievals from all three methods\n",
    "        regular_docs = regular_chain.invoke(question)\n",
    "        multiquery_docs = multiquery_chain.invoke({\"question\": question})\n",
    "        ragfusion_docs = ragfusion_chain.invoke({\"question\": question})\n",
    "        \n",
    "        # Calculate metrics\n",
    "        regular_metrics = calculate_retrieval_metrics(\n",
    "            regular_docs, question, embeddings\n",
    "        )\n",
    "        multiquery_metrics = calculate_retrieval_metrics(\n",
    "            multiquery_docs, question, embeddings\n",
    "        )\n",
    "        ragfusion_metrics = calculate_retrieval_metrics(\n",
    "            ragfusion_docs, question, embeddings\n",
    "        )\n",
    "        \n",
    "        # Get answers from all three methods\n",
    "        rag_chain = prompt | llm | StrOutputParser()\n",
    "        \n",
    "        regular_answer = rag_chain.invoke({\n",
    "            \"context\": \"\\n\\n\".join(doc.page_content for doc in regular_docs),\n",
    "            \"question\": question\n",
    "        })\n",
    "        \n",
    "        multiquery_answer = rag_chain.invoke({\n",
    "            \"context\": \"\\n\\n\".join(doc.page_content for doc in multiquery_docs),\n",
    "            \"question\": question\n",
    "        })\n",
    "        \n",
    "        fusion_answer = rag_chain.invoke({\n",
    "            \"context\": \"\\n\\n\".join(doc[0].page_content for doc in ragfusion_docs[:4]),\n",
    "            \"question\": question\n",
    "        })\n",
    "        \n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"regular\": {\n",
    "                \"metrics\": regular_metrics,\n",
    "                \"answer\": regular_answer,\n",
    "                \"top_chunks\": [doc.page_content[:200] + \"...\" for doc in regular_docs[:3]]\n",
    "            },\n",
    "            \"multiquery\": {\n",
    "                \"metrics\": multiquery_metrics,\n",
    "                \"answer\": multiquery_answer,\n",
    "                \"top_chunks\": [doc.page_content[:200] + \"...\" for doc in multiquery_docs[:3]]\n",
    "            },\n",
    "            \"ragfusion\": {\n",
    "                \"metrics\": ragfusion_metrics,\n",
    "                \"answer\": fusion_answer,\n",
    "                \"top_chunks\": [doc[0].page_content[:200] + \"...\" for doc in ragfusion_docs[:3]]\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Test questions\n",
    "Questions = [\n",
    "    \"What is tuition cost for the program?\",\n",
    "    \"What scholarships are available for the program?\",\n",
    "    \"What are the minimum scores for the TOEFL and IELTS English Language Requirement?\",\n",
    "    \"Is there an application fee waiver?\"\n",
    "]\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = compare_three_retrieval_methods(\n",
    "    Questions,\n",
    "    regular_retrieval_chain,\n",
    "    retrieval_chain_multiquery,\n",
    "    retrieval_chain_ragfusion,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "# Print detailed results\n",
    "for result in eval_results:\n",
    "    print(f\"\\nQuestion: {result['question']}\")\n",
    "    \n",
    "    print(\"\\nRegular RAG Metrics:\")\n",
    "    print(f\"Precision@4: {result['regular']['metrics'].precision_at_k:.3f}\")\n",
    "    print(f\"Recall@4: {result['regular']['metrics'].recall_at_k:.3f}\")\n",
    "    print(f\"MRR: {result['regular']['metrics'].mean_reciprocal_rank:.3f}\")\n",
    "    print(f\"Avg Relevance: {result['regular']['metrics'].relevance_score:.3f}\")\n",
    "    print(\"\\nAnswer:\", result['regular']['answer'])\n",
    "    \n",
    "    print(\"\\nMulti-Query Metrics:\")\n",
    "    print(f\"Precision@4: {result['multiquery']['metrics'].precision_at_k:.3f}\")\n",
    "    print(f\"Recall@4: {result['multiquery']['metrics'].recall_at_k:.3f}\")\n",
    "    print(f\"MRR: {result['multiquery']['metrics'].mean_reciprocal_rank:.3f}\")\n",
    "    print(f\"Avg Relevance: {result['multiquery']['metrics'].relevance_score:.3f}\")\n",
    "    print(\"\\nAnswer:\", result['multiquery']['answer'])\n",
    "    \n",
    "    print(\"\\nRAG-Fusion Metrics:\")\n",
    "    print(f\"Precision@4: {result['ragfusion']['metrics'].precision_at_k:.3f}\")\n",
    "    print(f\"Recall@4: {result['ragfusion']['metrics'].recall_at_k:.3f}\")\n",
    "    print(f\"MRR: {result['ragfusion']['metrics'].mean_reciprocal_rank:.3f}\")\n",
    "    print(f\"Avg Relevance: {result['ragfusion']['metrics'].relevance_score:.3f}\")\n",
    "    print(\"\\nAnswer:\", result['ragfusion']['answer'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Calculate average metrics across all questions\n",
    "avg_metrics = {\n",
    "    \"regular\": {\n",
    "        \"precision\": np.mean([r['regular']['metrics'].precision_at_k for r in eval_results]),\n",
    "        \"recall\": np.mean([r['regular']['metrics'].recall_at_k for r in eval_results]),\n",
    "        \"mrr\": np.mean([r['regular']['metrics'].mean_reciprocal_rank for r in eval_results]),\n",
    "        \"relevance\": np.mean([r['regular']['metrics'].relevance_score for r in eval_results])\n",
    "    },\n",
    "    \"multiquery\": {\n",
    "        \"precision\": np.mean([r['multiquery']['metrics'].precision_at_k for r in eval_results]),\n",
    "        \"recall\": np.mean([r['multiquery']['metrics'].recall_at_k for r in eval_results]),\n",
    "        \"mrr\": np.mean([r['multiquery']['metrics'].mean_reciprocal_rank for r in eval_results]),\n",
    "        \"relevance\": np.mean([r['multiquery']['metrics'].relevance_score for r in eval_results])\n",
    "    },\n",
    "    \"ragfusion\": {\n",
    "        \"precision\": np.mean([r['ragfusion']['metrics'].precision_at_k for r in eval_results]),\n",
    "        \"recall\": np.mean([r['ragfusion']['metrics'].recall_at_k for r in eval_results]),\n",
    "        \"mrr\": np.mean([r['ragfusion']['metrics'].mean_reciprocal_rank for r in eval_results]),\n",
    "        \"relevance\": np.mean([r['ragfusion']['metrics'].relevance_score for r in eval_results])\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nAverage Metrics Across All Questions:\")\n",
    "for method in [\"regular\", \"multiquery\", \"ragfusion\"]:\n",
    "    print(f\"\\n{method.title()} RAG:\")\n",
    "    print(f\"Avg Precision@4: {avg_metrics[method]['precision']:.3f}\")\n",
    "    print(f\"Avg Recall@4: {avg_metrics[method]['recall']:.3f}\")\n",
    "    print(f\"Avg MRR: {avg_metrics[method]['mrr']:.3f}\")\n",
    "    print(f\"Avg Relevance: {avg_metrics[method]['relevance']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51a1055b-e4b9-48c4-8b50-7d7125c29dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metrics Across All Questions:\n",
      "\n",
      "Regular RAG:\n",
      "Avg Precision@4: 0.750\n",
      "Avg Recall@4: 0.300\n",
      "Avg MRR: 1.000\n",
      "Avg Relevance: 0.850\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Multiquery RAG:\n",
      "Avg Precision@4: 0.542\n",
      "Avg Recall@4: 0.217\n",
      "Avg MRR: 1.000\n",
      "Avg Relevance: 0.841\n",
      "Accuracy: 0.00%\n",
      "\n",
      "Ragfusion RAG:\n",
      "Avg Precision@4: 0.438\n",
      "Avg Recall@4: 0.175\n",
      "Avg MRR: 1.000\n",
      "Avg Relevance: 0.842\n",
      "Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Ground truth answers for comparison\n",
    "ground_truth = {\n",
    "    \"What is tuition cost for the program?\": \"Tuition for the MS in Applied Data Science program: $5,967 per course/$71,604 total tuition\",\n",
    "    \"What scholarships are available for the program?\": \"The Data Science Institute Scholarship, MS in Applied Data Science Alumni Scholarship\",\n",
    "    \"What are the minimum scores for the TOEFL and IELTS English Language Requirement?\": \"Minimum scores for the Master’s in Applied Data Science program: TOEFL, 102 (no subscore requirement); IELTS, 7 (no subscore requirement).\",\n",
    "    \"Is there an application fee waiver?\": \"For questions regarding an application fee waiver, please refer to the Physical Sciences Division fee waiver policy.\",\n",
    "    \"What are the deadlines for the in-person program?\": \"November 7, 2024 – Priority Application Deadline\\nDecember 4, 2024 – Scholarship Priority Deadline\\nJanuary 21, 2025 – International Application Deadline (requiring visa sponsorship from UChicago)\\nMarch 4, 2025 – Second Priority Application Deadline\\nMay 6, 2025 – Third Priority Application Deadline\\nJune 23, 2025 – Final Application Deadline\",\n",
    "    \"How long will it take for me to receive a decision on my application?\": \"In-Person application decisions are released approximately 1 to 2 months after each respected deadline. Online application decisions are released on a rolling basis\",\n",
    "    \"Can I set up an advising appointment with the enrollment management team?\": \"Yes, meet your admissions counselor by scheduling an appointment https://apply-psd.uchicago.edu/portal/applied-data-science\",\n",
    "    \"Where can I mail my official transcripts?\": \"The University of Chicago\\nAttention: MS in Applied Data Science Admissions\\n455 N Cityfront Plaza Dr., Suite 950\\nChicago, Illinois 60611\",\n",
    "    \"Does the Master’s in Applied Data Science Online program provide visa sponsorship?\": \"Only our In-Person, Full-Time program is Visa eligible\",\n",
    "    \"How do I apply to the MBA/MS program?\": \"Applicants interested in the Joint MBA/MS degree will apply through Booth’s centralized, joint-application process. Applicants should complete the Chicago Booth Full-Time MBA application and select the MBA/MS in Applied Data Science as their program of interest\",\n",
    "    \"Is the MS in Applied Data Science program STEM/OPT eligible?\": \"The MS in Applied Data Science program is STEM/OPT eligible\",\n",
    "    \"How many courses must you complete to earn UChicago’s Master’s in Applied Data Science?\": \"To earn the MS-ADS degree students must successfully complete 12 courses (6 core, 4 elective, 2 Capstone) and our tailored Career Seminar.\"\n",
    "}\n",
    "\n",
    "# Adjusted evaluation function to compare answers to the ground truth\n",
    "def compare_retrieval_methods_with_ground_truth(questions, regular_chain, multiquery_chain, ragfusion_chain, embeddings, ground_truth):\n",
    "    \"\"\"Compare all three retrieval methods against ground truth using multiple metrics\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    for question in questions:\n",
    "        # Get retrievals from all three methods\n",
    "        regular_docs = regular_chain.invoke(question)\n",
    "        multiquery_docs = multiquery_chain.invoke({\"question\": question})\n",
    "        ragfusion_docs = ragfusion_chain.invoke({\"question\": question})\n",
    "        \n",
    "        # Calculate metrics\n",
    "        regular_metrics = calculate_retrieval_metrics(regular_docs, question, embeddings)\n",
    "        multiquery_metrics = calculate_retrieval_metrics(multiquery_docs, question, embeddings)\n",
    "        ragfusion_metrics = calculate_retrieval_metrics(ragfusion_docs, question, embeddings)\n",
    "        \n",
    "        # Get answers and compare with ground truth\n",
    "        rag_chain = prompt | llm | StrOutputParser()\n",
    "        \n",
    "        regular_answer = rag_chain.invoke({\"context\": \"\\n\\n\".join(doc.page_content for doc in regular_docs), \"question\": question})\n",
    "        multiquery_answer = rag_chain.invoke({\"context\": \"\\n\\n\".join(doc.page_content for doc in multiquery_docs), \"question\": question})\n",
    "        fusion_answer = rag_chain.invoke({\"context\": \"\\n\\n\".join(doc[0].page_content for doc in ragfusion_docs[:4]), \"question\": question})\n",
    "        \n",
    "        # Add comparison results\n",
    "        results.append({\n",
    "            \"regular\": {\"metrics\": regular_metrics, \"is_correct\": regular_answer.strip() == ground_truth[question]},\n",
    "            \"multiquery\": {\"metrics\": multiquery_metrics, \"is_correct\": multiquery_answer.strip() == ground_truth[question]},\n",
    "            \"ragfusion\": {\"metrics\": ragfusion_metrics, \"is_correct\": fusion_answer.strip() == ground_truth[question]}\n",
    "        })\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Compute average metrics only\n",
    "def calculate_average_metrics(eval_results):\n",
    "    avg_metrics = {\"regular\": {}, \"multiquery\": {}, \"ragfusion\": {}}\n",
    "    for method in avg_metrics:\n",
    "        avg_metrics[method] = {\n",
    "            \"precision\": np.mean([r[method][\"metrics\"].precision_at_k for r in eval_results]),\n",
    "            \"recall\": np.mean([r[method][\"metrics\"].recall_at_k for r in eval_results]),\n",
    "            \"mrr\": np.mean([r[method][\"metrics\"].mean_reciprocal_rank for r in eval_results]),\n",
    "            \"relevance\": np.mean([r[method][\"metrics\"].relevance_score for r in eval_results]),\n",
    "            \"accuracy\": np.mean([r[method][\"is_correct\"] for r in eval_results]) * 100\n",
    "        }\n",
    "    return avg_metrics\n",
    "\n",
    "# Test with the updated questions and answers\n",
    "Questions = list(ground_truth.keys())\n",
    "\n",
    "eval_results = compare_retrieval_methods_with_ground_truth(\n",
    "    Questions,\n",
    "    regular_retrieval_chain,\n",
    "    retrieval_chain_multiquery,\n",
    "    retrieval_chain_ragfusion,\n",
    "    embeddings,\n",
    "    ground_truth\n",
    ")\n",
    "\n",
    "# Calculate average metrics and print\n",
    "avg_metrics = calculate_average_metrics(eval_results)\n",
    "\n",
    "print(\"\\nAverage Metrics Across All Questions:\")\n",
    "for method, metrics in avg_metrics.items():\n",
    "    print(f\"\\n{method.title()} RAG:\")\n",
    "    print(f\"Avg Precision@4: {metrics['precision']:.3f}\")\n",
    "    print(f\"Avg Recall@4: {metrics['recall']:.3f}\")\n",
    "    print(f\"Avg MRR: {metrics['mrr']:.3f}\")\n",
    "    print(f\"Avg Relevance: {metrics['relevance']:.3f}\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536c5b50-d768-46e8-962a-af7360a338de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "env_jason",
   "name": ".m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m125"
  },
  "kernelspec": {
   "display_name": "env_jason (Local)",
   "language": "python",
   "name": "env_jason"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
